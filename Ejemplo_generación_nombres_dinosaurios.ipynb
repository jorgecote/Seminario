{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgecote/Seminario/blob/main/Ejemplo_generaci%C3%B3n_nombres_dinosaurios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxHgK-mEkqPO"
      },
      "source": [
        "# Generación de nombres de dinosaurios\n",
        "Desarrollar una red neuronal que pueda generar nombres de dinosaurios utilizando como entrada una letra o varias letras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nusx1jVjm4-8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(5)\n",
        "from keras.layers import Input, Dense, SimpleRNN\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXIVl6qrmafn"
      },
      "source": [
        "#### Carga de dataset\n",
        "El dataset contiene 1536 nombres diferentes de verdaderos dinosaurios organizados en un archivo de texto plano .txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3gxR6GqpKFr",
        "outputId": "aa7e3803-3dcd-4244-9174-8f02e4460c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')   #Se monta el drive para tener disponibles todos los archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9Z58Z_esp5Gk"
      },
      "outputs": [],
      "source": [
        "# Se utiliza la función open (Nativa de python) para abrir el archivo y read para leerlos\n",
        "nombres = open('/content/drive/MyDrive/ejercicios seminario internacional/nombres_dinosaurios.txt','r').read()\n",
        "nombres = nombres.lower() # Todos los nombres se dejan en minúscula para que no afecten las secuencias de generación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0qVh6mBsDSs",
        "outputId": "9976ad11-4deb-4fd4-9b45-a7fc03f30b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El total del texto es: 19909 caracteres\n",
            "aachenosaurus\n",
            "aardonyx\n",
            "abdallahsaurus\n",
            "\n",
            "Total de caracteres:  19909 , Caracteres únicos:  27\n",
            "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "#tam_datos, tam_alfabeto = len(nombres), len(alfabeto)\n",
        "#nombres=nombres.splitlines()\n",
        "print(f'El total del texto es: {len(nombres)} caracteres')    # Se calcula el total de caracteres del archivo\n",
        "print(nombres[:38])                                           # Revisando como luce el conjunto de datos\n",
        "alfabeto = sorted(set(nombres))                                 #Con la función set python encuentra los caracteres únicos del archivo\n",
        "print(\"Total de caracteres: \",len(nombres),\", Caracteres únicos: \", len(alfabeto))\n",
        "print(alfabeto)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcEdepHsxD3a"
      },
      "source": [
        "#### Codificación del dataset\n",
        "Primero se realiza la codificación asignando un número entero a cada caracter único en el conjunto de datos (archivo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKSsZmc1vWyS",
        "outputId": "a0cca504-86e2-4adf-ab1e-7fe73e1b75de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(19909,), dtype=string, numpy=array([b'a', b'a', b'c', ..., b'u', b'u', b'l'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "chars = tf.strings.unicode_split(nombres, input_encoding='UTF-8')   #Se separan los caracteres y se organizan en un tensor\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV3ECX7hxVzT",
        "outputId": "e6e4c4a3-6d40-46ab-c7eb-2b908eea5586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 2  2  4 ... 22 22 13], shape=(19909,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "car_a_ind = tf.keras.layers.StringLookup(vocabulary=alfabeto, mask_token=None)\n",
        "#{ car:ind for ind,car in enumerate(sorted(alfabeto))} #Se genera un diccionario asignando un valor único a cada caracter\n",
        "car_a_ind(chars)\n",
        "car_a_ind\n",
        "print(car_a_ind(chars))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1GqB0VDv7BP"
      },
      "source": [
        "#### Decodificación del dataset\n",
        "Se crea una función que ejecute lo contrario, para poder recuperar los caracteres a partir de los índices numéricos:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjYvsqI11JUK",
        "outputId": "9221b3b2-9516-4b9d-d7ba-90fa8c652f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'a' b'a' b'c' b'h' b'e' b'n' b'o' b's' b'a' b'u' b'r' b'u' b's' b'\\n'\n",
            " b'a' b'a' b'r' b'd' b'o' b'n' b'y' b'x' b'\\n' b'a' b'b' b'd' b'a' b'l'\n",
            " b'l' b'a' b'h' b's' b'a' b'u' b'r' b'u' b's' b'\\n'], shape=(38,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "ind_a_car = tf.keras.layers.StringLookup(vocabulary=car_a_ind.get_vocabulary(), invert=True, mask_token=None)\n",
        "print(ind_a_car(car_a_ind(chars))[:38])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wpvi0oIxee7"
      },
      "source": [
        "El conjunto de datos se puede reconstruir de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePKx5pdExRd6",
        "outputId": "3ad2ffcd-ab9e-4a3d-ac60-50f176ef3b8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'aachenosaurus\\naardonyx\\nabdallahsaurus\\nabelisaurus\\nabrictosaurus\\nabrosaurus\\nabydosaurus\\nacanthopholis\\nachelousaurus\\nacheroraptor\\nachillesaurus\\nachillobator\\nacristavus\\nacrocanthosaurus\\nacrotholus\\nactiosaurus\\nadamantisaurus\\nadasaurus\\nadelolophus\\nadeopapposaurus\\naegyptosaurus\\naeolosaurus\\naepisaurus\\naepyornithomimus\\naerosteon\\naetonyxafromimus\\nafrovenator\\nagathaumas\\naggiosaurus\\nagilisaurus\\nagnosphitys\\nagrosaurus\\nagujaceratops\\nagustinia\\nahshislepelta\\nairakoraptor\\najancingenia\\najkaceratops\\nalamosaurus\\nalaskacephale\\nalbalophosaurus\\nalbertaceratops\\nalbertadromeus\\nalbertavenator\\nalbertonykus\\nalbertosaurus\\nalbinykus\\nalbisaurus\\nalcovasaurus\\nalectrosaurus\\naletopelta\\nalgoasaurus\\nalioramus\\naliwalia\\nallosaurus\\nalmas\\nalnashetri\\nalocodon\\naltirhinus\\naltispinax\\nalvarezsaurus\\nalwalkeria\\nalxasaurus\\namargasaurus\\namargastegos\\namargatitanis\\namazonsaurus\\nammosaurus\\nampelosaurus\\namphicoelias\\namphicoelicaudia\\namphisaurus\\namtocephale\\namtosaurus\\namurosaurus\\namygdalodon\\nanabisetia\\nanasazisaurus\\nanatosaurus\\nanatotitan\\nanchiceratops\\nanchiornis\\nanchisaurus\\nandesaurus\\nandhrasaurus\\nangaturama\\nangloposeidon\\nangolatitan\\nangulomastacator\\naniksosaurus\\nanimantarx\\nankistrodon\\nankylosaurus\\nanodontosaurus\\nanoplosaurus\\nanserimimus\\nantarctopelta\\nantarctosaurus\\nantetonitrus\\nanthodon\\nantrodemus\\nanzu\\naoniraptor\\naorun\\napatodon\\napatoraptor\\napatosaurus\\nappalachiosaurus\\naquilops\\naragosaurus\\naralosaurus\\naraucanoraptor\\narchaeoceratops\\narchaeodontosaurus\\narchaeopteryx\\narchaeoraptor\\narchaeornis\\narchaeornithoides\\narchaeornithomimus\\narcovenator\\narctosaurus\\narcusaurus\\narenysaurus\\nargentinosaurus\\nargyrosaurus\\naristosaurus\\naristosuchus\\narizonasaurus\\narkansaurus\\narkharavia\\narrhinoceratops\\narstanosaurus\\nasiaceratops\\nasiamericana\\nasiatosaurus\\nastrodon\\nastrodonius\\nastrodontaurus\\nastrophocaudia\\nasylosaurus\\natacamatitan\\natlantosaurus\\natlasaurus\\natlascopcosaurus\\natrociraptor\\natsinganosaurus\\naublysodon\\naucasaurus\\naugustia\\naugustynolophus\\nauroraceratops\\naurornis\\naustralodocus\\naustralovenator\\naustrocheirus\\naustroposeidon\\naustroraptor\\naustrosaurus\\navaceratops\\navalonia\\navalonianus\\naviatyrannis\\navimimus\\navisaurus\\navipes\\nazendohsaurus\\nbactrosaurus\\nbagaceratops\\nbagaraatan\\nbahariasaurus\\nbainoceratops\\nbakesaurus\\nbalaur\\nbalochisaurus\\nbambiraptor\\nbanji\\nbaotianmansaurus\\nbarapasaurus\\nbarilium\\nbarosaurus\\nbarrosasaurus\\nbarsboldia\\nbaryonyx\\nbashunosaurus\\nbasutodon\\nbathygnathus\\nbatyrosaurus\\nbaurutitan\\nbayosaurus\\nbecklespinax\\nbeelemodon\\nbeibeilong\\nbeipiaognathus\\nbeipiaosaurus\\nbeishanlong\\nbellusaurus\\nbelodon\\nberberosaurus\\nbetasuchus\\nbicentenaria\\nbienosaurus\\nbihariosaurus\\nbilbeyhallorum\\nbissektipelta\\nbistahieversor\\nblancocerosaurus\\nblasisaurus\\nblikanasaurus\\nbolong\\nbonapartenykus\\nbonapartesaurus\\nbonatitan\\nbonitasaura\\nborealopelta\\nborealosaurus\\nboreonykus\\nborogovia\\nbothriospondylus\\nbrachiosaurus\\nbrachyceratops\\nbrachylophosaurus\\nbrachypodosaurus\\nbrachyrophus\\nbrachytaenius\\nbrachytrachelopan\\nbradycneme\\nbrasileosaurus\\nbrasilotitan\\nbravoceratops\\nbreviceratops\\nbrohisaurus\\nbrontomerus\\nbrontoraptor\\nbrontosaurus\\nbruhathkayosaurus\\nbugenasaura\\nbuitreraptor\\nburianosaurus\\nburiolestes\\nbyranjaffia\\nbyronosaurus\\ncaenagnathasia\\ncaenagnathus\\ncalamosaurus\\ncalamospondylus\\ncalamospondylus\\ncallovosaurus\\ncamarasaurus\\ncamarillasaurus\\ncamelotia\\ncamposaurus\\ncamptonotus\\ncamptosaurus\\ncampylodon\\ncampylodoniscus\\ncanardia\\ncapitalsaurus\\ncarcharodontosaurus\\ncardiodon\\ncarnotaurus\\ncaseosaurus\\ncathartesaura\\ncathetosaurus\\ncaudipteryx\\ncaudocoelus\\ncaulodon\\ncedarosaurus\\ncedarpelta\\ncedrorestes\\ncentemodon\\ncentrosaurus\\ncerasinops\\nceratonykus\\nceratops\\nceratosaurus\\ncetiosauriscus\\ncetiosaurus\\nchangchunsaurus\\nchangdusaurus\\nchangyuraptor\\nchaoyangsaurus\\ncharonosaurus\\nchasmosaurus\\nchassternbergia\\nchebsaurus\\nchenanisaurus\\ncheneosaurus\\nchialingosaurus\\nchiayusaurus\\nchienkosaurus\\nchihuahuasaurus\\nchilantaisaurus\\nchilesaurus\\nchindesaurus\\nchingkankousaurus\\nchinshakiangosaurus\\nchirostenotes\\nchoconsaurus\\nchondrosteosaurus\\nchromogisaurus\\nchuandongocoelurus\\nchuanjiesaurus\\nchuanqilong\\nchubutisaurus\\nchungkingosaurus\\nchuxiongosaurus\\ncinizasaurus\\ncionodon\\ncitipati\\ncladeiodon\\nclaorhynchus\\nclaosaurus\\nclarencea\\nclasmodosaurus\\nclepsysaurus\\ncoahuilaceratops\\ncoelophysis\\ncoelosaurus\\ncoeluroides\\ncoelurosauravus\\ncoelurus\\ncolepiocephale\\ncoloradia\\ncoloradisaurus\\ncolossosaurus\\ncomahuesaurus\\ncomanchesaurus\\ncompsognathus\\ncompsosuchus\\nconcavenator\\nconchoraptor\\ncondorraptor\\ncoronosaurus\\ncorythoraptor\\ncorythosaurus\\ncraspedodon\\ncrataeomus\\ncraterosaurus\\ncreosaurus\\ncrichtonpelta\\ncrichtonsaurus\\ncristatusaurus\\ncrosbysaurus\\ncruxicheiros\\ncryolophosaurus\\ncryptodraco\\ncryptoraptor\\ncryptosaurus\\ncryptovolans\\ncumnoria\\ndaanosaurus\\ndacentrurus\\ndachongosaurus\\ndaemonosaurus\\ndahalokely\\ndakosaurus\\ndakotadon\\ndakotaraptor\\ndaliansaurus\\ndamalasaurus\\ndandakosaurus\\ndanubiosaurus\\ndaptosaurus\\ndarwinsaurus\\ndashanpusaurus\\ndaspletosaurus\\ndasygnathoides\\ndasygnathus\\ndatanglong\\ndatonglong\\ndatousaurus\\ndaurosaurus\\ndaxiatitan\\ndeinocheirus\\ndeinodon\\ndeinonychus\\ndelapparentia\\ndeltadromeus\\ndemandasaurus\\ndenversaurus\\ndeuterosaurus\\ndiabloceratops\\ndiamantinasaurus\\ndianchungosaurus\\ndiceratops\\ndiceratusdiclonius\\ndicraeosaurus\\ndidanodondilong\\ndilophosaurus\\ndiluvicursor\\ndimodosaurus\\ndinheirosaurus\\ndinodocus\\ndinotyrannus\\ndiplodocus\\ndiplotomodon\\ndiracodon\\ndolichosuchus\\ndollodon\\ndomeykosaurus\\ndongbeititan\\ndongyangopelta\\ndongyangosaurus\\ndoratodon\\ndoryphorosaurus\\ndraconyx\\ndracopelta\\ndracoraptor\\ndracorex\\ndracovenator\\ndravidosaurus\\ndreadnoughtus\\ndrinker\\ndromaeosauroides\\ndromaeosaurus\\ndromiceiomimus\\ndromicosaurus\\ndrusilasaura\\ndryosaurus\\ndryptosauroides\\ndryptosaurus\\ndubreuillosaurus\\nduriatitan\\nduriavenator\\ndynamosaurus\\ndyoplosaurus\\ndysalotosaurus\\ndysganus\\ndyslocosaurus\\ndystrophaeus\\ndystylosaurus\\nechinodon\\nedmarka\\nedmontonia\\nedmontosaurus\\nefraasia\\neiniosaurus\\nekrixinatosaurus\\nelachistosuchus\\nelaltitan\\nelaphrosaurus\\nelmisaurus\\nelopteryx\\nelosaurus\\nelrhazosaurus\\nelvisaurus\\nemausaurus\\nembasaurus\\nenigmosaurus\\neoabelisaurus\\neobrontosaurus\\neocarcharia\\neoceratops\\neocursor\\neodromaeus\\neohadrosaurus\\neolambia\\neomamenchisaurus\\neoplophysis\\neoraptor\\neosinopteryx\\neotrachodon\\neotriceratops\\neotyrannus\\neousdryosaurus\\nepachthosaurus\\nepanterias\\nephoenosaurus\\nepicampodon\\nepichirostenotes\\nepidendrosaurus\\nepidexipteryx\\nequijubus\\nerectopus\\nerketu\\nerliansaurus\\nerlikosaurus\\neshanosaurus\\neuacanthus\\neucamerotus\\neucentrosaurus\\neucercosaurus\\neucnemesaurus\\neucoelophysis\\neugongbusaurus\\neuhelopus\\neuoplocephalus\\neupodosaurus\\neureodon\\neurolimnornis\\neuronychodon\\neuropasaurus\\neuropatitan\\neuropelta\\neuskelosaurus\\neustreptospondylus\\nfabrosaurus\\nfalcarius\\nfendusaurus\\nfenestrosaurus\\nferganasaurus\\nferganastegos\\nferganocephale\\nforaminacephale\\nfosterovenator\\nfrenguellisaurus\\nfruitadens\\nfukuiraptor\\nfukuisaurus\\nfukuititan\\nfukuivenator\\nfulengia\\nfulgurotherium\\nfusinasus\\nfusuisaurus\\nfutabasaurus\\nfutalognkosaurus\\ngadolosaurus\\ngaleamopus\\ngalesaurus\\ngallimimus\\ngaltonia\\ngalveosaurus\\ngalvesaurus\\ngannansaurus\\ngansutitan\\nganzhousaurus\\ngargoyleosaurus\\ngarudimimus\\ngasosaurus\\ngasparinisaura\\ngastonia\\ngavinosaurus\\ngeminiraptor\\ngenusaurus\\ngenyodectes\\ngeranosaurus\\ngideonmantellia\\ngiganotosaurus\\ngigantoraptor\\ngigantosaurus\\ngigantosaurus\\ngigantoscelus\\ngigantspinosaurus\\ngilmoreosaurus\\nginnareemimus\\ngiraffatitan\\nglacialisaurus\\nglishades\\nglyptodontopelta\\nskeleton\\ngobiceratops\\ngobisaurus\\ngobititan\\ngobivenator\\ngodzillasaurus\\ngojirasaurus\\ngondwanatitan\\ngongbusaurus\\ngongpoquansaurus\\ngongxianosaurus\\ngorgosaurus\\ngoyocephale\\ngraciliceratops\\ngraciliraptor\\ngracilisuchus\\ngravitholus\\ngresslyosaurus\\ngriphornis\\ngriphosaurus\\ngryphoceratops\\ngryponyx\\ngryposaurus\\ngspsaurus\\nguaibasaurus\\ngualicho\\nguanlong\\ngwyneddosaurus\\ngyposaurus\\nhadrosauravus\\nhadrosaurus\\nhaestasaurus\\nhagryphus\\nhallopus\\nhalszkaraptor\\nhalticosaurus\\nhanssuesia\\nhanwulosaurus\\nhaplocanthosaurus\\nhaplocanthus\\nhaplocheirus\\nharpymimus\\nhaya\\nhecatasaurus\\nheilongjiangosaurus\\nheishansaurus\\nhelioceratops\\nhelopus\\nheptasteornis\\nherbstosaurus\\nherrerasaurus\\nhesperonychus\\nhesperosaurus\\nheterodontosaurus\\nheterosaurus\\nhexing\\nhexinlusaurus\\nheyuannia\\nhierosaurus\\nhippodraco\\nhironosaurus\\nhisanohamasaurus\\nhistriasaurus\\nhomalocephale\\nhonghesaurus\\nhongshanosaurus\\nhoplitosaurus\\nhoplosaurus\\nhorshamosaurus\\nhortalotarsus\\nhuabeisaurus\\nhualianceratops\\nhuanansaurus\\nhuanghetitan\\nhuangshanlong\\nhuaxiagnathus\\nhuaxiaosaurus\\nhuaxiasaurus\\nhuayangosaurus\\nhudiesaurus\\nhuehuecanauhtlus\\nhulsanpes\\nhungarosaurus\\nhuxleysaurus\\nhylaeosaurus\\nhylosaurushypacrosaurus\\nhypselorhachis\\nhypselosaurus\\nhypselospinus\\nhypsibema\\nhypsilophodon\\nhypsirhophus\\nhabodcraniosaurus\\nichthyovenator\\nignavusaurus\\niguanacolossus\\niguanodon\\niguanoides\\nskeleton\\niguanosaurus\\niliosuchus\\nilokelesia\\nincisivosaurus\\nindosaurus\\nindosuchus\\ningenia\\ninosaurus\\nirritator\\nisaberrysaura\\nisanosaurus\\nischioceratops\\nischisaurus\\nischyrosaurus\\nisisaurus\\nissasaurus\\nitemirus\\niuticosaurus\\njainosaurus\\njaklapallisaurus\\njanenschia\\njaxartosaurus\\njeholosaurus\\njenghizkhan\\njensenosaurus\\njeyawati\\njianchangosaurus\\njiangjunmiaosaurus\\njiangjunosaurus\\njiangshanosaurus\\njiangxisaurus\\njianianhualong\\njinfengopteryx\\njingshanosaurus\\njintasaurus\\njinzhousaurus\\njiutaisaurus\\njobaria\\njubbulpuria\\njudiceratops\\njurapteryx\\njurassosaurus\\njuratyrant\\njuravenator\\nkagasaurus\\nkaijiangosaurus\\nkakuru\\nkangnasaurus\\nkarongasaurus\\nkatepensaurus\\nkatsuyamasaurus\\nkayentavenator\\nkazaklambia\\nkelmayisaurus\\nkemkemiakentrosaurus\\nkentrurosaurus\\nkerberosaurus\\nkentrosaurus\\nkhaan\\nkhetranisaurus\\nkileskus\\nkinnareemimus\\nkitadanisaurus\\nkittysaurus\\nklamelisauruskol\\nkoparion\\nkoreaceratops\\nkoreanosaurus\\nkoreanosaurus\\nkoshisaurus\\nkosmoceratops\\nkotasaurus\\nkoutalisaurus\\nkritosaurus\\nkryptops\\nkrzyzanowskisaurus\\nkukufeldia\\nkulceratops\\nkulindadromeus\\nkulindapteryx\\nkunbarrasaurus\\nkundurosaurus\\nkunmingosaurus\\nkuszholia\\nlabocania\\nlabrosaurus\\nlaelaps\\nlaevisuchus\\nlagerpeton\\nlagosuchus\\nlaiyangosaurus\\nlamaceratops\\nlambeosaurus\\nlametasaurus\\nlamplughsaura\\nlanasaurus\\nlancangosaurus\\nlancanjiangosaurus\\nlanzhousaurus\\nlaosaurus\\nlapampasaurus\\nlaplatasaurus\\nlapparentosaurus\\nlaquintasaura\\nlatenivenatrix\\nlatirhinus\\nleaellynasaura\\nleinkupal\\nleipsanosaurus\\nlengosaurus\\nleonerasaurus\\nlepidocheirosaurus\\nlepidus\\nleptoceratops\\nleptorhynchos\\nleptospondylus\\nleshansaurus\\nlesothosaurus\\nlessemsaurus\\nlevnesovia\\nlewisuchus\\nlexovisaurus\\nleyesaurus\\nliaoceratops\\nliaoningosaurus\\nliaoningtitan\\nliaoningvenator\\nliassaurus\\nlibycosaurus\\nligabueino\\nligabuesaurus\\nligomasaurus\\nlikhoelesaurus\\nliliensternus\\nlimaysaurus\\nlimnornis\\nlimnosaurus\\nlimusaurus\\nlinhenykus\\nlinheraptor\\nlinhevenator\\nlirainosaurus\\nlisboasaurusliubangosaurus\\nlohuecotitan\\nloncosaurus\\nlongisquama\\nlongosaurus\\nlophorhothon\\nlophostropheus\\nloricatosaurus\\nloricosaurus\\nlosillasaurus\\nlourinhanosaurus\\nlourinhasaurus\\nluanchuanraptor\\nluanpingosaurus\\nlucianosaurus\\nlucianovenator\\nlufengosaurus\\nlukousaurus\\nluoyanggia\\nlurdusaurus\\nlusitanosaurus\\nlusotitan\\nlycorhinus\\nlythronax\\nmacelognathus\\nmachairasaurus\\nmachairoceratops\\nmacrodontophion\\nmacrogryphosaurus\\nmacrophalangia\\nmacroscelosaurus\\nmacrurosaurus\\nmadsenius\\nmagnapaulia\\nmagnamanus\\nmagnirostris\\nmagnosaurus\\nmagulodon\\nmagyarosaurus\\nmahakala\\nmaiasaura\\nmajungasaurus\\nmajungatholus\\nmalarguesaurus\\nmalawisaurus\\nmaleevosaurus\\nmaleevus\\nmamenchisaurus\\nmanidens\\nmandschurosaurus\\nmanospondylus\\nmantellisaurus\\nmantellodon\\nmapusaurus\\nmarasuchus\\nmarisaurus\\nmarmarospondylus\\nmarshosaurus\\nmartharaptor\\nmasiakasaurus\\nmassospondylus\\nmatheronodon\\nmaxakalisaurus\\nmedusaceratops\\nmegacervixosaurus\\nmegadactylus\\nmegadontosaurus\\nmegalosaurus\\nmegapnosaurus\\nmegaraptor\\nmei\\nmelanorosaurus\\nmendozasaurus\\nmercuriceratops\\nmeroktenos\\nmetriacanthosaurus\\nmicrocephale\\nmicroceratops\\nmicroceratus\\nmicrocoelus\\nmicrodontosaurus\\nmicrohadrosaurus\\nmicropachycephalosaurus\\nmicroraptor\\nmicrovenator\\nmierasaurus\\nmifunesaurus\\nminmi\\nminotaurasaurus\\nmiragaia\\nmirischia\\nmoabosaurus\\nmochlodon\\nmohammadisaurus\\nmojoceratops\\nmongolosaurus\\nmonkonosaurus\\nmonoclonius\\nmonolophosaurus\\nmononychus\\nmononykus\\nmontanoceratops\\nmorelladon\\nmorinosaurus\\nmorosaurus\\nmorrosaurus\\nmosaiceratops\\nmoshisaurus\\nmtapaiasaurus\\nmtotosaurus\\nmurusraptor\\nmussaurus\\nmuttaburrasaurus\\nmuyelensaurus\\nmymoorapelta\\nnaashoibitosaurus\\nnambalia\\nnankangia\\nnanningosaurus\\nnanosaurus\\nnanotyrannus\\nnanshiungosaurus\\nnanuqsaurus\\nnanyangosaurus\\nnarambuenatitan\\nnasutoceratops\\nnatronasaurus\\nnebulasaurus\\nnectosaurus\\nnedcolbertia\\nnedoceratops\\nneimongosaurus\\nnemegtia\\nnemegtomaia\\nnemegtosaurus\\nneosaurus\\nneosodon\\nneovenator\\nneuquenraptor\\nneuquensaurus\\nnewtonsaurus\\nngexisaurus\\nnicksaurus\\nnigersaurus\\nningyuansaurus\\nniobrarasaurus\\nnipponosaurus\\nnoasaurus\\nnodocephalosaurus\\nnodosaurus\\nnomingia\\nnopcsaspondylus\\nnormanniasaurus\\nnothronychus\\nnotoceratops\\nnotocolossus\\nnotohypsilophodon\\nnqwebasaurus\\nnteregosaurus\\nnurosaurus\\nnuthetes\\nnyasasaurus\\nnyororosaurus\\nohmdenosaurus\\nojoceratops\\nojoraptorsaurus\\noligosaurus\\nolorotitan\\nomeisaurus\\nomosaurus\\nonychosaurus\\noohkotokia\\nopisthocoelicaudia\\noplosaurus\\norcomimus\\norinosaurusorkoraptor\\nornatotholusornithodesmus\\nornithoides\\nornitholestes\\nornithomerus\\nornithomimoides\\nornithomimus\\nornithopsis\\nornithosuchus\\nornithotarsus\\norodromeus\\norosaurus\\northogoniosaurus\\northomerus\\noryctodromeus\\noshanosaurus\\nosmakasaurus\\nostafrikasaurus\\nostromia\\nothnielia\\nothnielosaurus\\notogosaurus\\nouranosaurus\\noverosaurus\\noviraptor\\novoraptor\\nowenodon\\noxalaia\\nozraptor\\npachycephalosaurus\\npachyrhinosaurus\\npachysauriscus\\npachysaurops\\npachysaurus\\npachyspondylus\\npachysuchus\\npadillasaurus\\npakisaurus\\npalaeoctonus\\npalaeocursornis\\npalaeolimnornis\\npalaeopteryx\\npalaeosauriscus\\npalaeosaurus\\npalaeosaurus\\npalaeoscincus\\npaleosaurus\\npaludititan\\npaluxysaurus\\npampadromaeus\\npamparaptor\\npanamericansaurus\\npandoravenator\\npanguraptor\\npanoplosaurus\\npanphagia\\npantydraco\\nparaiguanodon\\nparalititan\\nparanthodon\\npararhabdodon\\nparasaurolophus\\npareiasaurus\\nparksosaurus\\nparonychodon\\nparrosaurus\\nparvicursor\\npatagonykus\\npatagosaurus\\npatagotitan\\npawpawsaurus\\npectinodon\\npedopenna\\npegomastax\\npeishansaurus\\npekinosaurus\\npelecanimimus\\npellegrinisaurus\\npeloroplites\\npelorosaurus\\npeltosaurus\\npenelopognathus\\npentaceratops\\npetrobrasaurus\\nphaedrolosaurus\\nphilovenator\\nphuwiangosaurus\\nphyllodon\\npiatnitzkysaurus\\npicrodon\\npinacosaurus\\npisanosaurus\\npitekunsaurus\\npiveteausaurus\\nplanicoxa\\nplateosauravus\\nplateosaurus\\nplatyceratops\\nplesiohadros\\npleurocoelus\\npleuropeltus\\npneumatoarthrus\\npneumatoraptor\\npodokesaurus\\npoekilopleuron\\npolacanthoides\\npolacanthus\\npolyodontosaurus\\npolyonax\\nponerosteus\\npoposaurus\\nparasaurolophus\\npostosuchus\\npowellvenator\\npradhania\\nprenocephale\\nprenoceratops\\npriconodon\\npriodontognathus\\nproa\\nprobactrosaurus\\nprobrachylophosaurus\\nproceratops\\nproceratosaurus\\nprocerosaurus\\nprocerosaurus\\nprocheneosaurus\\nprocompsognathus\\nprodeinodon\\nproiguanodon\\npropanoplosaurus\\nproplanicoxa\\nprosaurolophus\\nprotarchaeopteryx\\nprotecovasaurus\\nprotiguanodon\\nprotoavis\\nprotoceratops\\nprotognathosaurus\\nprotognathus\\nprotohadros\\nprotorosaurus\\nprotorosaurus\\nprotrachodon\\nproyandusaurus\\npseudolagosuchus\\npsittacosaurus\\npteropelyx\\npterospondylus\\npuertasaurus\\npukyongosaurus\\npulanesaura\\npycnonemosaurus\\npyroraptor\\nqantassaurus\\nqianzhousaurus\\nqiaowanlong\\nqijianglong\\nqinlingosaurus\\nqingxiusaurus\\nqiupalong\\nquaesitosaurus\\nquetecsaurus\\nquilmesaurus\\nrachitrema\\nrahiolisaurus\\nrahona\\nrahonavis\\nrajasaurus\\nrapator\\nrapetosaurus\\nraptorex\\nratchasimasaurus\\nrativates\\nrayososaurus\\nrazanandrongobe\\nrebbachisaurus\\nregaliceratops\\nregnosaurus\\nrevueltosaurus\\nrhabdodon\\nrhadinosaurus\\nrhinorex\\nrhodanosaurus\\nrhoetosaurus\\nrhopalodon\\nriabininohadros\\nrichardoestesia\\nrileya\\nrileyasuchus\\nrinchenia\\nrinconsaurus\\nrioarribasaurus\\nriodevasaurus\\nriojasaurus\\nriojasuchus\\nrocasaurus\\nroccosaurus\\nrubeosaurus\\nruehleia\\nrugocaudia\\nrugops\\nrukwatitan\\nruyangosaurus\\nsacisaurus\\nsahaliyania\\nsaichania\\nsaldamosaurus\\nsalimosaurus\\nsaltasaurus\\nsaltopus\\nsaltriosaurus\\nsanchusaurus\\nsangonghesaurus\\nsanjuansaurus\\nsanpasaurus\\nsantanaraptor\\nsaraikimasoom\\nsarahsaurus\\nsarcolestes\\nsarcosaurus\\nsarmientosaurus\\nsaturnalia\\nsauraechinodon\\nsaurolophus\\nsauroniops\\nsauropelta\\nsaurophaganax\\nsaurophagus\\nsauroplites\\nsauroposeidon\\nsaurornithoides\\nsaurornitholestes\\nsavannasaurus\\nscansoriopteryx\\nscaphonyx\\nscelidosaurus\\nscipionyx\\nsciurumimus\\nscleromochlus\\nscolosaurus\\nscutellosaurus\\nsecernosaurus\\nsefapanosaurus\\nsegisaurus\\nsegnosaurus\\nseismosaurus\\nseitaad\\nselimanosaurus\\nsellacoxa\\nsellosaurus\\nserendipaceratops\\nserikornis\\nshamosaurus\\nshanag\\nshanshanosaurus\\nshantungosaurus\\nshanxia\\nshanyangosaurus\\nshaochilong\\nshenzhousaurus\\nshidaisaurus\\nshingopana\\nshixinggia\\nshuangbaisaurus\\nshuangmiaosaurus\\nshunosaurus\\nshuvosaurus\\nshuvuuia\\nsiamodon\\nsiamodracon\\nsiamosaurus\\nsiamotyrannus\\nsiats\\nsibirosaurus\\nsibirotitan\\nsidormimus\\nsigilmassasaurus\\nsilesaurus\\nsiluosaurus\\nsilvisaurus\\nsimilicaudipteryx\\nsinocalliopteryx\\nsinoceratops\\nsinocoelurus\\nsinopelta\\nsinopeltosaurus\\nsinornithoides\\nsinornithomimus\\nsinornithosaurus\\nsinosauropteryx\\nsinosaurus\\nsinotyrannus\\nsinovenator\\nsinraptor\\nsinusonasus\\nsirindhorna\\nskorpiovenator\\nsmilodon\\nsonidosaurus\\nsonorasaurus\\nsoriatitan\\nsphaerotholus\\nsphenosaurus\\nsphenospondylus\\nspiclypeus\\nspinophorosaurus\\nspinops\\nspinosaurus\\nspinostropheus\\nspinosuchus\\nspondylosoma\\nsqualodon\\nstaurikosaurus\\nstegoceras\\nstegopelta\\nstegosaurides\\nstegosaurus\\nstenonychosaurus\\nstenopelix\\nstenotholus\\nstephanosaurus\\nstereocephalus\\nsterrholophus\\nstokesosaurus\\nstormbergia\\nstrenusaurus\\nstreptospondylus\\nstruthiomimus\\nstruthiosaurus\\nstygimoloch\\nstygivenator\\nstyracosaurus\\nsuccinodon\\nsuchomimus\\nsuchosaurus\\nsuchoprion\\nsugiyamasaurus\\nskeleton\\nsulaimanisaurus\\nsupersaurus\\nsuuwassea\\nsuzhousaurus\\nsymphyrophus\\nsyngonosaurus\\nsyntarsus\\nsyrmosaurus\\nszechuanosaurus\\ntachiraptor\\ntalarurus\\ntalenkauen\\ntalos\\ntambatitanis\\ntangvayosaurus\\ntanius\\ntanycolagreus\\ntanystropheus\\ntanystrosuchus\\ntaohelong\\ntapinocephalus\\ntapuiasaurus\\ntarascosaurus\\ntarbosaurus\\ntarchia\\ntastavinsaurus\\ntatankacephalus\\ntatankaceratops\\ntataouinea\\ntatisaurus\\ntaurovenator\\ntaveirosaurus\\ntawa\\ntawasaurus\\ntazoudasaurus\\ntechnosaurus\\ntecovasaurus\\ntehuelchesaurus\\nteihivenator\\nteinurosaurus\\nteleocrater\\ntelmatosaurus\\ntenantosaurus\\ntenchisaurus\\ntendaguria\\ntengrisaurus\\ntenontosaurus\\nteratophoneus\\nteratosaurus\\ntermatosaurus\\ntethyshadros\\ntetragonosaurus\\ntexacephale\\ntexasetes\\nteyuwasu\\nthecocoelurus\\nthecodontosaurus\\nthecospondylus\\ntheiophytalia\\ntherizinosaurus\\ntherosaurus\\nthescelosaurus\\nthespesius\\nthotobolosaurus\\ntianchisaurus\\ntianchungosaurus\\ntianyulong\\ntianyuraptor\\ntianzhenosaurus\\ntichosteus\\ntienshanosaurus\\ntimimus\\ntimurlengia\\ntitanoceratops\\ntitanosaurus\\ntitanosaurus\\ntochisaurus\\ntomodon\\ntonganosaurus\\ntongtianlong\\ntonouchisaurus\\ntorilion\\ntornieria\\ntorosaurus\\ntorvosaurus\\ntototlmimus\\ntrachodon\\ntraukutitan\\ntrialestes\\ntriassolestes\\ntribelesodon\\ntriceratops\\ntrigonosaurus\\ntrimucrodon\\ntrinisaura\\ntriunfosaurus\\ntroodon\\ntsaagan\\ntsagantegia\\ntsintaosaurus\\ntugulusaurus\\ntuojiangosaurus\\nturanoceratops\\nturiasaurus\\ntylocephale\\ntylosteus\\ntyrannosaurus\\ntyrannotitan\\nillustration\\nuberabatitan\\nudanoceratops\\nugrosaurus\\nugrunaaluk\\nuintasaurus\\nultrasauros\\nultrasaurus\\nultrasaurus\\numarsaurus\\nunaysaurus\\nunenlagia\\nunescoceratops\\nunicerosaurus\\nunquillosaurus\\nurbacodon\\nutahceratops\\nutahraptor\\nuteodon\\nvagaceratops\\nvahiny\\nvaldoraptor\\nvaldosaurus\\nvariraptor\\nvelociraptor\\nvectensia\\nvectisaurus\\nvelafrons\\nvelocipes\\nvelociraptor\\nvelocisaurus\\nvenaticosuchus\\nvenenosaurus\\nveterupristisaurus\\nviavenator\\nvitakridrinda\\nvitakrisaurus\\nvolkheimeria\\nvouivria\\nvulcanodon\\nwadhurstia\\nwakinosaurus\\nwalgettosuchus\\nwalkeria\\nwalkersaurus\\nwangonisaurus\\nwannanosaurus\\nwellnhoferia\\nwendiceratops\\nwiehenvenator\\nwillinakaqe\\nwintonotitan\\nwuerhosaurus\\nwulagasaurus\\nwulatelong\\nwyleyia\\nwyomingraptor\\nxenoceratops\\nxenoposeidon\\nxenotarsosaurus\\nxianshanosaurus\\nxiaosaurus\\nxingxiulong\\nxinjiangovenator\\nxinjiangtitan\\nxiongguanlong\\nxixianykus\\nxixiasaurus\\nxixiposaurus\\nxuanhanosaurus\\nxuanhuaceratops\\nxuanhuasaurus\\nxuwulong\\nyaleosaurus\\nyamaceratops\\nyandusaurus\\nyangchuanosaurus\\nyaverlandia\\nyehuecauhceratops\\nyezosaurus\\nyibinosaurus\\nyimenosaurus\\nyingshanosaurus\\nyinlong\\nyixianosaurus\\nyizhousaurus\\nyongjinglong\\nyuanmouraptor\\nyuanmousaurus\\nyueosaurus\\nyulong\\nyunganglong\\nyunmenglong\\nyunnanosaurus\\nyunxianosaurus\\nyurgovuchia\\nyutyrannus\\nzanabazar\\nzanclodon\\nzapalasaurus\\nzapsalis\\nzaraapelta\\nzatomuszby\\nzephyrosaurus\\nzhanghenglong\\nzhejiangosaurus\\nzhenyuanlong\\nzhongornis\\nzhongjianosaurus\\nzhongyuansaurus\\nzhuchengceratops\\nzhuchengosaurus\\nzhuchengtitan\\nzhuchengtyrannus\\nziapelta\\nzigongosaurus\\nzizhongosaurus\\nzuniceratops\\nzunityrannus\\nzuolong\\nzuoyunlong\\nzupaysaurus\\nzuul'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UiUOqmURqS5S"
      },
      "outputs": [],
      "source": [
        "def ind_a_texto(ind):           #Función para recuperar cadena de texto desde índices\n",
        "  return tf.strings.reduce_join(ind_a_car(ind), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "624ZEMs3h9Eb"
      },
      "source": [
        "#### Creación de entradas y salidas\n",
        "Para la creación de las entradas y salidas para el entrenamiento se debe dividir el conjunto con los caracteres desde la posición 0 a tamaño-1 y las salidas desde 1 a tamaño.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "Si la cadena es 'Hello'\n",
        "\n",
        "La entrada sería: 'Hell'\n",
        "\n",
        "La salida sería: 'ello'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6LsncOWjJzS"
      },
      "source": [
        "Para esto primero organizamos los datos en un dataset de tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sD8ggWQ7jU30"
      },
      "outputs": [],
      "source": [
        "all_ids = car_a_ind(tf.strings.unicode_split(nombres, 'UTF-8'))   #Separar los caracteres y asignandoles id único\n",
        "\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)       #Organizar los índices generados en un dataset de TensorFlow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlPbLsnRjvd_",
        "outputId": "822bcbfe-d962-466f-817a-343c5474ad2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n",
            "a\n",
            "c\n",
            "h\n",
            "e\n",
            "n\n",
            "o\n",
            "s\n",
            "a\n",
            "u\n",
            "r\n",
            "u\n",
            "s\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(14):        #Mostrando los primeros 14 caracteres del dataset\n",
        "    print(ind_a_car(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYJennv1nxSq"
      },
      "source": [
        "Con el conjunto de datos almacenado en el dataset ahora con nel método batch se puede realizar la separación utilizando como argumento el tamaño de la secuencia de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "28j9VWsko3HA"
      },
      "outputs": [],
      "source": [
        "seq_length = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80vdTG6CpFWK",
        "outputId": "3051b2d9-4a93-4024-e0df-300609e2355c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'a' b'a' b'c' b'h' b'e' b'n' b'o' b's' b'a' b'u' b'r' b'u' b's' b'\\n'\n",
            " b'a' b'a' b'r' b'd' b'o' b'n' b'y'], shape=(21,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'x' b'\\n' b'a' b'b' b'd' b'a' b'l' b'l' b'a' b'h' b's' b'a' b'u' b'r'\n",
            " b'u' b's' b'\\n' b'a' b'b' b'e' b'l'], shape=(21,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(2):\n",
        "  print(ind_a_car(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOzvgcj3qGWT"
      },
      "source": [
        "Si se unen de nuevo las secuencias lo que se obtiene es:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bygSCaTYqJw1",
        "outputId": "b34cc562-3bae-478a-a653-8449786af264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'aachenosaurus\\naardony'\n",
            "b'x\\nabdallahsaurus\\nabel'\n",
            "b'isaurus\\nabrictosaurus'\n",
            "b'\\nabrosaurus\\nabydosaur'\n",
            "b'us\\nacanthopholis\\nache'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(ind_a_texto(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v-J9fqyrWcb"
      },
      "source": [
        "Organizando el conjunto para tener entradas y salidas donde ambos son secuencias. Por medio de la siguiente función se pretende obtener [input label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "i8pGgaEKsIJz"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):       #Recibe una secuencia como argumento\n",
        "    input_text = sequence[:-1]          #Se toman los primeros n-1 caracteres de la secuencia\n",
        "    target_text = sequence[1:]          #Se toman los caracteres del 1 al n de la secuencia\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwhKsqAlvPYC"
      },
      "source": [
        "Ahora se aplica a todas las secuencias generadas, a traves del método map de python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "i0fXRkXzvYKv"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYhQ3Uq10Fc8"
      },
      "source": [
        "Como ejemplo se muestra el resultado de [input, target] a partir de la separación explicada arriba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwW2_i3zvsOH",
        "outputId": "483b8c06-a3d5-4d7c-d8f3-f41da25e88bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'aachenosaurus\\naardon'\n",
            "Target: b'achenosaurus\\naardony'\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", ind_a_texto(input_example).numpy())\n",
        "    print(\"Target:\", ind_a_texto(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPy-yYXN0975"
      },
      "source": [
        "Antes de iniciar el entrenamiento se empaquetará el conjunto por lotes (batches) y revolver las secuencias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY7S7Iqt1pa2",
        "outputId": "c19b426c-716d-4d03-aa5a-8512d8eb87ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(20, 20), dtype=tf.int64, name=None), TensorSpec(shape=(20, 20), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "BATCH_SIZE = 20               # Batch size\n",
        "\n",
        "BUFFER_SIZE = 10000           #Para revolver (shuffle) las secuencias se recomienda hacerlo en un buffer de tamaño fijo para no ocupar completamente la memoria\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBDX-8K_HEUR"
      },
      "source": [
        "#### Construcción del modelo\n",
        "A continuación se construye el modelo a partir de clases heredadas, el cual tendrá la siguiente estructura:\n",
        "\n",
        "\n",
        "\n",
        "*   Capa embedding: Para evitar el one-hot-ncoding y aumentar innecesariamente la dimensionalidad del conjunto\n",
        "*   Capa recurrente: Se realizarán modelos con rnn simple, LSTM y GRU\n",
        "*   Capa densa: se encargará de ponderar y entregar el resultado de la genración de texto\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyzw0go6IlNy"
      },
      "source": [
        "Se inicializan los parámetros necesarios para las 3 capas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZAS1TMOIqMG",
        "outputId": "70c79f53-b939-49c6-f6f9-f87ace88f5ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ],
      "source": [
        "# Longitud del vocabulario (caracteres únicos)\n",
        "vocab_size = len(car_a_ind.get_vocabulary())\n",
        "print(vocab_size)\n",
        "# The embedding dimension\n",
        "embedding_dim = 5\n",
        "\n",
        "# Number of RNN units\n",
        "lstm_units = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jGRCNPPNCkdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c4cc1f-be63-4bc6-fc58-7b66e3609ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"First-LSTM-Model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 5)           140       \n",
            "                                                                 \n",
            " Hidden-Recurrent-Layer (LS  (None, None, 15)          1260      \n",
            " TM)                                                             \n",
            "                                                                 \n",
            " Hidden-Layer (Dense)        (None, None, 28)          448       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1848 (7.22 KB)\n",
            "Trainable params: 1848 (7.22 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "inpu=tf.keras.Input(shape=(None,))            #Capa de entrada\n",
        "model_sec = Sequential(name=\"First-LSTM-Model\") # Creación de Modelo\n",
        "#model.add(Input(shape=(time_step,1), name='Input-Layer')) # Input Layer - need to speicfy the shape of inputs\n",
        "model_sec.add(tf.keras.Input(shape=(None,)))\n",
        "model_sec.add(tf.keras.layers.Embedding(vocab_size, embedding_dim))\n",
        "#if states is None:\n",
        "  #model_sec.add(tf.keras.layers.LSTM(units=lstm_units,return_states=True, name='Hidden-Recurrent-Layer').get_initial_state()) # Hidden Recurrent Layer, Tanh(x) = sinh(x)/cosh(x) = ((exp(x) - exp(-x))/(exp(x) + exp(-x)))\n",
        "model_sec.add(tf.keras.layers.LSTM(units=lstm_units, return_sequences=True, name='Hidden-Recurrent-Layer')) # Hidden Recurrent Layer, Tanh(x) = sinh(x)/cosh(x) = ((exp(x) - exp(-x))/(exp(x) + exp(-x)))\n",
        "model_sec.add(Dense(units=vocab_size,  name='Hidden-Layer')) # Hidden Layer, Tanh(x) = sinh(x)/cosh(x) = ((exp(x) - exp(-x))/(exp(x) + exp(-x)))\n",
        "#model.add(Dense(units=1, activation='linear', name='Output-Layer')) # Output Layer, Linear(x) = x\n",
        "model_sec.compile(optimizer='adam', # default='rmsprop', an algorithm to be used in backpropagation\n",
        "              loss='mean_squared_error', # Loss function to be optimized. A string (name of loss function), or a tf.keras.losses.Loss instance.\n",
        "              metrics=['MeanSquaredError', 'MeanAbsoluteError'], # List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance.\n",
        "              loss_weights=None, # default=None, Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs.\n",
        "              weighted_metrics=None, # default=None, List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing.\n",
        "              run_eagerly=None, # Defaults to False. If True, this Model's logic will not be wrapped in a tf.function. Recommended to leave this as None unless your Model cannot be run inside a tf.function.\n",
        "              steps_per_execution=None # Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead.\n",
        "             )\n",
        "pred_train = model_sec(inpu)\n",
        "model_sec.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuZgv2ayN-Hr",
        "outputId": "eaf41432-d12d-4032-f890-6a2d2201e8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "salida de modelo API secuencial (20, 20, 28) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model_sec(input_example_batch)\n",
        "    print(\"salida de modelo API secuencial\",example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sXNBEjpjo_B"
      },
      "source": [
        "#### Realizando la primera predicción\n",
        "A la salida se obtiene una distribución de probabilidad donde la salida con mayor probabilidad sera la neurona activa. Para mostrar la primera predicción sin entrenamiento, se tomará una de manera aleatoria y se organiza en un solo vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JaEX75UYj_Vo"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E_WQB-El6vw",
        "outputId": "4d2f73f1-8d05-4c25-ad55-f8f7efd2a090"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19, 19,  3, 24, 21, 27,  5, 22, 14, 10, 23, 18, 20, 25,  7, 17, 24,\n",
              "        7,  2,  2])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_Dw_lENN1F_"
      },
      "source": [
        "Al decodificar el resultado obtenido:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkZtFoGBmf5D",
        "outputId": "7051ef18-e288-4074-8f65-23c217e74865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " [b'i' b'a' b'n' b's' b'a' b'u' b'r' b'u' b's' b'\\n' b'e' b'r' b'l' b'i'\n",
            " b'k' b'o' b's' b'a' b'u' b'r']\n",
            "\n",
            "Next Char Predictions:\n",
            " b'rrbwtzdumivqsxfpwfaa'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", ind_a_car(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", ind_a_texto(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgkC-G3WnF-Y"
      },
      "source": [
        "#### Entrenamiento del modelo\n",
        "Despues de realizar todo el preprocesamiento del conjunto de datos el problema se puede tratar como un problema típico de clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqbkL0a3og1C"
      },
      "source": [
        "Se utilizara la función de pérdidas que usa la entropís cruzada que es bastante usada para los problemas de clasificación. Teniendo en cuenta que el modelo entrega valores de probabilidad (logits) se activa el argumento from_logits=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Tbg9JrxNpGMb"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRRnd4oOpUho"
      },
      "source": [
        "Aún sin entrenar el modelo tiene unas pérdidas promedio de 3.33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjh_WhXIpOHp",
        "outputId": "0f7fef03-67b6-4f54-8a5e-6053e7b43250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (20, 20, 28)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(3.3321848, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8vMy5O2pkxg"
      },
      "source": [
        "Teniendo en cuenta que el modelo está recién inicializado, al calcular e elevado a las pérdidas promedio se debería obtener un número cercano al tamaño del vocabulario. Si resulta mucho más grande esto indica que el modelo no está inicializado apropiadamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D10IF9ZpimW",
        "outputId": "782d2c36-4ca7-4ebb-a401-32343f6695c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27.999449"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zazhAxp0qeM5"
      },
      "source": [
        "Ahora se compilará el modelo utilizando el optimizador Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_sec.compile(optimizer='adam', loss=loss)\n",
        "history_sec = model_sec.fit(dataset, epochs=400)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3qEQbuvw1uG",
        "outputId": "84ac587f-81c3-4d7b-95bf-6e42cb4e91f7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "47/47 [==============================] - 3s 22ms/step - loss: 3.2740\n",
            "Epoch 2/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 3.0179\n",
            "Epoch 3/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.8841\n",
            "Epoch 4/400\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 2.8528\n",
            "Epoch 5/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.8361\n",
            "Epoch 6/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.8212\n",
            "Epoch 7/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.8042\n",
            "Epoch 8/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.7753\n",
            "Epoch 9/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.7368\n",
            "Epoch 10/400\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 2.6878\n",
            "Epoch 11/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.6375\n",
            "Epoch 12/400\n",
            "47/47 [==============================] - 1s 8ms/step - loss: 2.5911\n",
            "Epoch 13/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 2.5469\n",
            "Epoch 14/400\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 2.5022\n",
            "Epoch 15/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 2.4578\n",
            "Epoch 16/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 2.4186\n",
            "Epoch 17/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 2.3810\n",
            "Epoch 18/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 2.3463\n",
            "Epoch 19/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 2.3177\n",
            "Epoch 20/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 2.2903\n",
            "Epoch 21/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 2.2659\n",
            "Epoch 22/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.2396\n",
            "Epoch 23/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.2149\n",
            "Epoch 24/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.1927\n",
            "Epoch 25/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.1716\n",
            "Epoch 26/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.1503\n",
            "Epoch 27/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.1276\n",
            "Epoch 28/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.1092\n",
            "Epoch 29/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0941\n",
            "Epoch 30/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0790\n",
            "Epoch 31/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0646\n",
            "Epoch 32/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0546\n",
            "Epoch 33/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0446\n",
            "Epoch 34/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0407\n",
            "Epoch 35/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0284\n",
            "Epoch 36/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0221\n",
            "Epoch 37/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0161\n",
            "Epoch 38/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0091\n",
            "Epoch 39/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0023\n",
            "Epoch 40/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9965\n",
            "Epoch 41/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9892\n",
            "Epoch 42/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9833\n",
            "Epoch 43/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9785\n",
            "Epoch 44/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9741\n",
            "Epoch 45/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9665\n",
            "Epoch 46/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9612\n",
            "Epoch 47/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9591\n",
            "Epoch 48/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9537\n",
            "Epoch 49/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9500\n",
            "Epoch 50/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9451\n",
            "Epoch 51/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9422\n",
            "Epoch 52/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9377\n",
            "Epoch 53/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9350\n",
            "Epoch 54/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9288\n",
            "Epoch 55/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9300\n",
            "Epoch 56/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9246\n",
            "Epoch 57/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9220\n",
            "Epoch 58/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9207\n",
            "Epoch 59/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9171\n",
            "Epoch 60/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9167\n",
            "Epoch 61/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.9107\n",
            "Epoch 62/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.9096\n",
            "Epoch 63/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.9063\n",
            "Epoch 64/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.9037\n",
            "Epoch 65/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.9037\n",
            "Epoch 66/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.9009\n",
            "Epoch 67/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8989\n",
            "Epoch 68/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8954\n",
            "Epoch 69/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8937\n",
            "Epoch 70/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8910\n",
            "Epoch 71/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8910\n",
            "Epoch 72/400\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 1.8901\n",
            "Epoch 73/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8892\n",
            "Epoch 74/400\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 1.8853\n",
            "Epoch 75/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8845\n",
            "Epoch 76/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8814\n",
            "Epoch 77/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8813\n",
            "Epoch 78/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8803\n",
            "Epoch 79/400\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 1.8783\n",
            "Epoch 80/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8750\n",
            "Epoch 81/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8760\n",
            "Epoch 82/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8744\n",
            "Epoch 83/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8720\n",
            "Epoch 84/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8711\n",
            "Epoch 85/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8699\n",
            "Epoch 86/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8695\n",
            "Epoch 87/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8692\n",
            "Epoch 88/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8662\n",
            "Epoch 89/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8651\n",
            "Epoch 90/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8653\n",
            "Epoch 91/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8612\n",
            "Epoch 92/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8617\n",
            "Epoch 93/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8621\n",
            "Epoch 94/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8605\n",
            "Epoch 95/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8570\n",
            "Epoch 96/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8586\n",
            "Epoch 97/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8566\n",
            "Epoch 98/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.8559\n",
            "Epoch 99/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8541\n",
            "Epoch 100/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.8518\n",
            "Epoch 101/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.8520\n",
            "Epoch 102/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8510\n",
            "Epoch 103/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8484\n",
            "Epoch 104/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8521\n",
            "Epoch 105/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8478\n",
            "Epoch 106/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8493\n",
            "Epoch 107/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8437\n",
            "Epoch 108/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8461\n",
            "Epoch 109/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8462\n",
            "Epoch 110/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8442\n",
            "Epoch 111/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8425\n",
            "Epoch 112/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8399\n",
            "Epoch 113/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8397\n",
            "Epoch 114/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8388\n",
            "Epoch 115/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8385\n",
            "Epoch 116/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8375\n",
            "Epoch 117/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8368\n",
            "Epoch 118/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8375\n",
            "Epoch 119/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8361\n",
            "Epoch 120/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8353\n",
            "Epoch 121/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8340\n",
            "Epoch 122/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8358\n",
            "Epoch 123/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8348\n",
            "Epoch 124/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8325\n",
            "Epoch 125/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8283\n",
            "Epoch 126/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8290\n",
            "Epoch 127/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8309\n",
            "Epoch 128/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8291\n",
            "Epoch 129/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8278\n",
            "Epoch 130/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8273\n",
            "Epoch 131/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8261\n",
            "Epoch 132/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8253\n",
            "Epoch 133/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8239\n",
            "Epoch 134/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8222\n",
            "Epoch 135/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8199\n",
            "Epoch 136/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8212\n",
            "Epoch 137/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8218\n",
            "Epoch 138/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8202\n",
            "Epoch 139/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8187\n",
            "Epoch 140/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8177\n",
            "Epoch 141/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8175\n",
            "Epoch 142/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8157\n",
            "Epoch 143/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8156\n",
            "Epoch 144/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8144\n",
            "Epoch 145/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.8144\n",
            "Epoch 146/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8154\n",
            "Epoch 147/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8122\n",
            "Epoch 148/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8107\n",
            "Epoch 149/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8143\n",
            "Epoch 150/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8122\n",
            "Epoch 151/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.8092\n",
            "Epoch 152/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8085\n",
            "Epoch 153/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8105\n",
            "Epoch 154/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8071\n",
            "Epoch 155/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8077\n",
            "Epoch 156/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8055\n",
            "Epoch 157/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8059\n",
            "Epoch 158/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8036\n",
            "Epoch 159/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8046\n",
            "Epoch 160/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8031\n",
            "Epoch 161/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8035\n",
            "Epoch 162/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7997\n",
            "Epoch 163/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8006\n",
            "Epoch 164/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8002\n",
            "Epoch 165/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7983\n",
            "Epoch 166/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8000\n",
            "Epoch 167/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7986\n",
            "Epoch 168/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7959\n",
            "Epoch 169/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7950\n",
            "Epoch 170/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7971\n",
            "Epoch 171/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7948\n",
            "Epoch 172/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7946\n",
            "Epoch 173/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7956\n",
            "Epoch 174/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7921\n",
            "Epoch 175/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7929\n",
            "Epoch 176/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7905\n",
            "Epoch 177/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7904\n",
            "Epoch 178/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7917\n",
            "Epoch 179/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7901\n",
            "Epoch 180/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7871\n",
            "Epoch 181/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7879\n",
            "Epoch 182/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7866\n",
            "Epoch 183/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7883\n",
            "Epoch 184/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7878\n",
            "Epoch 185/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7839\n",
            "Epoch 186/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7864\n",
            "Epoch 187/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7848\n",
            "Epoch 188/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7850\n",
            "Epoch 189/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7835\n",
            "Epoch 190/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7829\n",
            "Epoch 191/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7821\n",
            "Epoch 192/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7829\n",
            "Epoch 193/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7782\n",
            "Epoch 194/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7814\n",
            "Epoch 195/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7822\n",
            "Epoch 196/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7776\n",
            "Epoch 197/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7791\n",
            "Epoch 198/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7767\n",
            "Epoch 199/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7771\n",
            "Epoch 200/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7762\n",
            "Epoch 201/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7774\n",
            "Epoch 202/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7764\n",
            "Epoch 203/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7732\n",
            "Epoch 204/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7757\n",
            "Epoch 205/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7718\n",
            "Epoch 206/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7721\n",
            "Epoch 207/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7715\n",
            "Epoch 208/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7711\n",
            "Epoch 209/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7710\n",
            "Epoch 210/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7700\n",
            "Epoch 211/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7712\n",
            "Epoch 212/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7680\n",
            "Epoch 213/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7686\n",
            "Epoch 214/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7680\n",
            "Epoch 215/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7679\n",
            "Epoch 216/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7659\n",
            "Epoch 217/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7676\n",
            "Epoch 218/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7667\n",
            "Epoch 219/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7657\n",
            "Epoch 220/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7636\n",
            "Epoch 221/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7633\n",
            "Epoch 222/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7627\n",
            "Epoch 223/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7637\n",
            "Epoch 224/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7642\n",
            "Epoch 225/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7626\n",
            "Epoch 226/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7606\n",
            "Epoch 227/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7610\n",
            "Epoch 228/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7605\n",
            "Epoch 229/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7603\n",
            "Epoch 230/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7594\n",
            "Epoch 231/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7568\n",
            "Epoch 232/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7600\n",
            "Epoch 233/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7578\n",
            "Epoch 234/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7583\n",
            "Epoch 235/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7571\n",
            "Epoch 236/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7569\n",
            "Epoch 237/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7557\n",
            "Epoch 238/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7541\n",
            "Epoch 239/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7546\n",
            "Epoch 240/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7534\n",
            "Epoch 241/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7519\n",
            "Epoch 242/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7537\n",
            "Epoch 243/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7517\n",
            "Epoch 244/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7523\n",
            "Epoch 245/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7522\n",
            "Epoch 246/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7515\n",
            "Epoch 247/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7532\n",
            "Epoch 248/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7502\n",
            "Epoch 249/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7503\n",
            "Epoch 250/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7492\n",
            "Epoch 251/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7489\n",
            "Epoch 252/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7483\n",
            "Epoch 253/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7497\n",
            "Epoch 254/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7467\n",
            "Epoch 255/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7463\n",
            "Epoch 256/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7469\n",
            "Epoch 257/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7489\n",
            "Epoch 258/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7475\n",
            "Epoch 259/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7470\n",
            "Epoch 260/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7451\n",
            "Epoch 261/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7431\n",
            "Epoch 262/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7446\n",
            "Epoch 263/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7444\n",
            "Epoch 264/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7418\n",
            "Epoch 265/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7412\n",
            "Epoch 266/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7446\n",
            "Epoch 267/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7428\n",
            "Epoch 268/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7416\n",
            "Epoch 269/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7435\n",
            "Epoch 270/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7410\n",
            "Epoch 271/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7414\n",
            "Epoch 272/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7411\n",
            "Epoch 273/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7393\n",
            "Epoch 274/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7430\n",
            "Epoch 275/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7414\n",
            "Epoch 276/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7386\n",
            "Epoch 277/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7410\n",
            "Epoch 278/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7386\n",
            "Epoch 279/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7394\n",
            "Epoch 280/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7378\n",
            "Epoch 281/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7366\n",
            "Epoch 282/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7365\n",
            "Epoch 283/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7377\n",
            "Epoch 284/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7360\n",
            "Epoch 285/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7371\n",
            "Epoch 286/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7358\n",
            "Epoch 287/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7364\n",
            "Epoch 288/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7349\n",
            "Epoch 289/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7356\n",
            "Epoch 290/400\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 1.7344\n",
            "Epoch 291/400\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 1.7380\n",
            "Epoch 292/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7337\n",
            "Epoch 293/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7326\n",
            "Epoch 294/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7327\n",
            "Epoch 295/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7336\n",
            "Epoch 296/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7321\n",
            "Epoch 297/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7321\n",
            "Epoch 298/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7314\n",
            "Epoch 299/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7323\n",
            "Epoch 300/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7324\n",
            "Epoch 301/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7320\n",
            "Epoch 302/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7324\n",
            "Epoch 303/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7333\n",
            "Epoch 304/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7297\n",
            "Epoch 305/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7296\n",
            "Epoch 306/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7293\n",
            "Epoch 307/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7291\n",
            "Epoch 308/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7289\n",
            "Epoch 309/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7291\n",
            "Epoch 310/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7305\n",
            "Epoch 311/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7294\n",
            "Epoch 312/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7292\n",
            "Epoch 313/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7298\n",
            "Epoch 314/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7267\n",
            "Epoch 315/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7294\n",
            "Epoch 316/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7272\n",
            "Epoch 317/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7267\n",
            "Epoch 318/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7259\n",
            "Epoch 319/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7264\n",
            "Epoch 320/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7254\n",
            "Epoch 321/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7261\n",
            "Epoch 322/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7266\n",
            "Epoch 323/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7246\n",
            "Epoch 324/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7258\n",
            "Epoch 325/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7251\n",
            "Epoch 326/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7244\n",
            "Epoch 327/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7237\n",
            "Epoch 328/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7237\n",
            "Epoch 329/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7240\n",
            "Epoch 330/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7232\n",
            "Epoch 331/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7230\n",
            "Epoch 332/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7253\n",
            "Epoch 333/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7210\n",
            "Epoch 334/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7233\n",
            "Epoch 335/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7224\n",
            "Epoch 336/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7228\n",
            "Epoch 337/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7235\n",
            "Epoch 338/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7205\n",
            "Epoch 339/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7194\n",
            "Epoch 340/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7226\n",
            "Epoch 341/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7206\n",
            "Epoch 342/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7217\n",
            "Epoch 343/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7213\n",
            "Epoch 344/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7174\n",
            "Epoch 345/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7218\n",
            "Epoch 346/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7205\n",
            "Epoch 347/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7179\n",
            "Epoch 348/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7184\n",
            "Epoch 349/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7204\n",
            "Epoch 350/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7178\n",
            "Epoch 351/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7177\n",
            "Epoch 352/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7172\n",
            "Epoch 353/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7187\n",
            "Epoch 354/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7193\n",
            "Epoch 355/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7198\n",
            "Epoch 356/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7172\n",
            "Epoch 357/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7172\n",
            "Epoch 358/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7173\n",
            "Epoch 359/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7156\n",
            "Epoch 360/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7154\n",
            "Epoch 361/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7152\n",
            "Epoch 362/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7147\n",
            "Epoch 363/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7157\n",
            "Epoch 364/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7168\n",
            "Epoch 365/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7165\n",
            "Epoch 366/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7168\n",
            "Epoch 367/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7180\n",
            "Epoch 368/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7155\n",
            "Epoch 369/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7158\n",
            "Epoch 370/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7148\n",
            "Epoch 371/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7126\n",
            "Epoch 372/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7152\n",
            "Epoch 373/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7140\n",
            "Epoch 374/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7146\n",
            "Epoch 375/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7133\n",
            "Epoch 376/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7133\n",
            "Epoch 377/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7118\n",
            "Epoch 378/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7125\n",
            "Epoch 379/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7116\n",
            "Epoch 380/400\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 1.7107\n",
            "Epoch 381/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7120\n",
            "Epoch 382/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7123\n",
            "Epoch 383/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7129\n",
            "Epoch 384/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7116\n",
            "Epoch 385/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7109\n",
            "Epoch 386/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7103\n",
            "Epoch 387/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7120\n",
            "Epoch 388/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7105\n",
            "Epoch 389/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7086\n",
            "Epoch 390/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7092\n",
            "Epoch 391/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7115\n",
            "Epoch 392/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7091\n",
            "Epoch 393/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7106\n",
            "Epoch 394/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7078\n",
            "Epoch 395/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7081\n",
            "Epoch 396/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7089\n",
            "Epoch 397/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7072\n",
            "Epoch 398/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7089\n",
            "Epoch 399/400\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7114\n",
            "Epoch 400/400\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 1.7087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHppzInAsQQU"
      },
      "source": [
        "#### Generación de texto\n",
        "\n",
        "La generación de texto se realizará corriendo el modelo en un loop. Cada vez que se llame el modelo recibirá texto y el estado anterior y la predicción será el siguiente caracter. En la siguiente iteración recibirá la predicción y el estado anterior para generar un nuevo caracter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_char = tf.constant(['die'])\n",
        "\n",
        "\n",
        "result = [next_char]\n",
        "states=None\n",
        "\n",
        "for n in range(50):\n",
        "  input_chars = tf.strings.unicode_split(next_char, 'UTF-8')\n",
        "  input_ids = car_a_ind(input_chars).to_tensor()\n",
        "  print(input_chars)\n",
        "  print(input_ids)\n",
        "\n",
        "  predicted_logits = model_sec(inputs=input_ids)\n",
        "  #print(predicted_logits)\n",
        "\n",
        "  predicted_logits = predicted_logits[:, -1, :]\n",
        "  #print(predicted_logits)\n",
        "  predicted_logits = predicted_logits/1.0\n",
        "  print(predicted_logits)\n",
        "\n",
        "  predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "  #print(predicted_ids.numpy())\n",
        "  predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "  print(predicted_ids.numpy())\n",
        "\n",
        "  next_char = ind_a_car(predicted_ids)\n",
        "  #print(next_char)\n",
        "  result.append(next_char)\n",
        "  #print(result)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSVQrH3l8AP2",
        "outputId": "2fe81ba8-9ed3-4d8a-d744-18d52f77bf4f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'd', b'i', b'e']]>\n",
            "tf.Tensor([[ 5 10  6]], shape=(1, 3), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-17.423586    -1.6461207   -0.6249049   -1.8495392   -0.32624078\n",
            "   -0.37992555  -1.368176    -4.2565722    1.3272929   -1.5641093\n",
            "    0.15492988  -2.2532423   -2.226986     1.6032658   -0.3582132\n",
            "    2.2883916    1.5658805    0.03693216  -4.1140623    0.41531602\n",
            "    0.26914495   0.65691113   0.7185998   -0.2178269   -3.1853158\n",
            "   -3.1254182   -0.6718692   -1.893971  ]], shape=(1, 28), dtype=float32)\n",
            "[23]\n",
            "<tf.RaggedTensor [[b'v']]>\n",
            "tf.Tensor([[23]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-13.26162     -1.8370492    1.9753497   -2.275848    -3.1375637\n",
            "   -2.0493405    3.3450794   -5.5925336   -4.442448    -0.2945557\n",
            "    2.6150093   -5.78706     -1.7675182   -0.5993976   -3.146029\n",
            "   -1.7811321    2.3561375   -0.7218998   -4.6728888    0.06532553\n",
            "    0.26472253  -0.6707453    2.0587225   -3.9262128   -4.621283\n",
            "   -4.3863044    0.91158724  -4.367721  ]], shape=(1, 28), dtype=float32)\n",
            "[16]\n",
            "<tf.RaggedTensor [[b'o']]>\n",
            "tf.Tensor([[16]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-15.078376    -2.394916    -1.128819    -0.8153734    1.2076231\n",
            "    1.3727484   -1.2957234   -2.6848135   -0.24540532  -1.2956328\n",
            "   -1.4852178   -1.8150665   -1.071228     1.2359458    0.79754174\n",
            "    2.350381    -1.7910467    1.8468916   -3.1194077    2.0386448\n",
            "    2.7928462    0.92800456  -1.6066785    0.16054189  -2.636657\n",
            "   -1.4418733   -2.2556834   -2.2305248 ]], shape=(1, 28), dtype=float32)\n",
            "[20]\n",
            "<tf.RaggedTensor [[b's']]>\n",
            "tf.Tensor([[20]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.922222     3.3873086    3.5913012   -2.272068     0.41525608\n",
            "   -1.1351376   -0.44606     -5.929862    -1.4398097   -0.8052802\n",
            "    0.30118203  -4.9426317   -0.6229373   -1.6500816   -1.2978677\n",
            "   -1.2194184   -0.02141809   0.6447394   -2.3076181   -3.5671203\n",
            "   -0.587002     1.2121404    1.5694895   -2.8460288   -6.041628\n",
            "   -3.0227447   -0.64159435  -1.606858  ]], shape=(1, 28), dtype=float32)\n",
            "[2]\n",
            "<tf.RaggedTensor [[b'a']]>\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.197617     0.9354166   -1.2216392   -0.38523966   0.5999329\n",
            "   -0.3640166   -0.5576194   -2.4707313   -1.089143    -1.864829\n",
            "   -1.0792776   -2.1646671   -1.33855      0.93801826  -0.23154995\n",
            "    1.9290271   -1.3502681    0.590977    -2.81739      0.9044548\n",
            "    0.37393165   1.3183277    2.24695     -0.1511141   -2.40418\n",
            "   -1.1641562   -2.8116326   -1.7073164 ]], shape=(1, 28), dtype=float32)\n",
            "[15]\n",
            "<tf.RaggedTensor [[b'n']]>\n",
            "tf.Tensor([[15]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-8.155976    1.1496396   1.3120141  -2.0969398  -0.57633686 -0.75516164\n",
            "   0.9588535  -2.6226768   1.8043219  -0.28766727  1.6556569  -1.4435362\n",
            "  -0.37079296 -0.95188653 -1.0807632   0.36082187  2.1566477  -1.1884615\n",
            "  -2.0435061  -2.7895901  -0.11317265  0.03925556  0.29689583 -2.2678285\n",
            "  -3.6995893  -1.325242    0.6385932  -1.390274  ]], shape=(1, 28), dtype=float32)\n",
            "[16]\n",
            "<tf.RaggedTensor [[b'o']]>\n",
            "tf.Tensor([[16]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-15.078376    -2.394916    -1.128819    -0.8153734    1.2076231\n",
            "    1.3727484   -1.2957234   -2.6848135   -0.24540532  -1.2956328\n",
            "   -1.4852178   -1.8150665   -1.071228     1.2359458    0.79754174\n",
            "    2.350381    -1.7910467    1.8468916   -3.1194077    2.0386448\n",
            "    2.7928462    0.92800456  -1.6066785    0.16054189  -2.636657\n",
            "   -1.4418733   -2.2556834   -2.2305248 ]], shape=(1, 28), dtype=float32)\n",
            "[17]\n",
            "<tf.RaggedTensor [[b'p']]>\n",
            "tf.Tensor([[17]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.985919    -4.606115     0.86873764  -1.4218582   -0.77844346\n",
            "   -1.387161     1.7894719   -3.2361908   -3.249292     2.1370964\n",
            "    0.49777627  -3.353333    -0.8638432   -0.1815663   -1.2671824\n",
            "   -1.9219204    1.7259024    0.51852655  -2.6600704    2.077378\n",
            "    1.7393904    1.252691    -0.6674669   -2.976428    -2.6207683\n",
            "   -3.9716492   -0.20765388  -2.7327456 ]], shape=(1, 28), dtype=float32)\n",
            "[19]\n",
            "<tf.RaggedTensor [[b'r']]>\n",
            "tf.Tensor([[19]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-14.599499     1.7202605    2.518543    -2.6529145   -2.1212134\n",
            "   -1.9656932    1.442919    -4.7754927   -1.8354521   -1.9313799\n",
            "    2.2760196   -4.9011083   -1.6018707   -0.55350477  -2.0070415\n",
            "    0.24730863   2.4657726   -1.8045192   -4.458078    -2.9736395\n",
            "   -0.5336878   -1.0396607    2.947944    -3.0768123   -5.585214\n",
            "   -2.6257782    0.60047376  -3.1338842 ]], shape=(1, 28), dtype=float32)\n",
            "[2]\n",
            "<tf.RaggedTensor [[b'a']]>\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.197617     0.9354166   -1.2216392   -0.38523966   0.5999329\n",
            "   -0.3640166   -0.5576194   -2.4707313   -1.089143    -1.864829\n",
            "   -1.0792776   -2.1646671   -1.33855      0.93801826  -0.23154995\n",
            "    1.9290271   -1.3502681    0.590977    -2.81739      0.9044548\n",
            "    0.37393165   1.3183277    2.24695     -0.1511141   -2.40418\n",
            "   -1.1641562   -2.8116326   -1.7073164 ]], shape=(1, 28), dtype=float32)\n",
            "[19]\n",
            "<tf.RaggedTensor [[b'r']]>\n",
            "tf.Tensor([[19]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-14.599499     1.7202605    2.518543    -2.6529145   -2.1212134\n",
            "   -1.9656932    1.442919    -4.7754927   -1.8354521   -1.9313799\n",
            "    2.2760196   -4.9011083   -1.6018707   -0.55350477  -2.0070415\n",
            "    0.24730863   2.4657726   -1.8045192   -4.458078    -2.9736395\n",
            "   -0.5336878   -1.0396607    2.947944    -3.0768123   -5.585214\n",
            "   -2.6257782    0.60047376  -3.1338842 ]], shape=(1, 28), dtype=float32)\n",
            "[20]\n",
            "<tf.RaggedTensor [[b's']]>\n",
            "tf.Tensor([[20]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.922222     3.3873086    3.5913012   -2.272068     0.41525608\n",
            "   -1.1351376   -0.44606     -5.929862    -1.4398097   -0.8052802\n",
            "    0.30118203  -4.9426317   -0.6229373   -1.6500816   -1.2978677\n",
            "   -1.2194184   -0.02141809   0.6447394   -2.3076181   -3.5671203\n",
            "   -0.587002     1.2121404    1.5694895   -2.8460288   -6.041628\n",
            "   -3.0227447   -0.64159435  -1.606858  ]], shape=(1, 28), dtype=float32)\n",
            "[17]\n",
            "<tf.RaggedTensor [[b'p']]>\n",
            "tf.Tensor([[17]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.985919    -4.606115     0.86873764  -1.4218582   -0.77844346\n",
            "   -1.387161     1.7894719   -3.2361908   -3.249292     2.1370964\n",
            "    0.49777627  -3.353333    -0.8638432   -0.1815663   -1.2671824\n",
            "   -1.9219204    1.7259024    0.51852655  -2.6600704    2.077378\n",
            "    1.7393904    1.252691    -0.6674669   -2.976428    -2.6207683\n",
            "   -3.9716492   -0.20765388  -2.7327456 ]], shape=(1, 28), dtype=float32)\n",
            "[19]\n",
            "<tf.RaggedTensor [[b'r']]>\n",
            "tf.Tensor([[19]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-14.599499     1.7202605    2.518543    -2.6529145   -2.1212134\n",
            "   -1.9656932    1.442919    -4.7754927   -1.8354521   -1.9313799\n",
            "    2.2760196   -4.9011083   -1.6018707   -0.55350477  -2.0070415\n",
            "    0.24730863   2.4657726   -1.8045192   -4.458078    -2.9736395\n",
            "   -0.5336878   -1.0396607    2.947944    -3.0768123   -5.585214\n",
            "   -2.6257782    0.60047376  -3.1338842 ]], shape=(1, 28), dtype=float32)\n",
            "[22]\n",
            "<tf.RaggedTensor [[b'u']]>\n",
            "tf.Tensor([[22]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-1.2161116e+01 -3.8603261e+00  1.1229653e+00 -1.4182469e+00\n",
            "   5.3376919e-01 -8.1922638e-01 -1.7297268e-03 -2.9635191e+00\n",
            "  -3.2978909e+00  6.0650003e-01 -4.9681973e-01 -4.0741353e+00\n",
            "  -1.0073954e+00  7.8386921e-01 -1.3137209e-01 -6.0228929e-02\n",
            "  -1.6640171e+00 -3.7713444e-01 -2.3042631e+00  3.1221511e+00\n",
            "   3.4416745e+00  7.4058604e-01  2.8342211e-01 -1.8877864e+00\n",
            "  -2.7571464e+00 -2.9800422e+00 -2.3175602e+00 -2.1210260e+00]], shape=(1, 28), dtype=float32)\n",
            "[20]\n",
            "<tf.RaggedTensor [[b's']]>\n",
            "tf.Tensor([[20]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.922222     3.3873086    3.5913012   -2.272068     0.41525608\n",
            "   -1.1351376   -0.44606     -5.929862    -1.4398097   -0.8052802\n",
            "    0.30118203  -4.9426317   -0.6229373   -1.6500816   -1.2978677\n",
            "   -1.2194184   -0.02141809   0.6447394   -2.3076181   -3.5671203\n",
            "   -0.587002     1.2121404    1.5694895   -2.8460288   -6.041628\n",
            "   -3.0227447   -0.64159435  -1.606858  ]], shape=(1, 28), dtype=float32)\n",
            "[1]\n",
            "<tf.RaggedTensor [[b'\\n']]>\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-7.0054893  -3.4786339   0.58242464  0.27924868  1.0223589   0.45471132\n",
            "   0.3861202  -0.29105878  0.72620726 -0.02788016 -0.8643983   0.17599824\n",
            "   0.27783486  0.01075115  0.8604859   0.12017788 -0.18005173  1.4274062\n",
            "  -0.60252404  0.23331997  1.5764735   1.1057353  -1.3854105  -0.73246586\n",
            "  -0.63410294  0.614537   -0.7077815  -0.36003038]], shape=(1, 28), dtype=float32)\n",
            "[6]\n",
            "<tf.RaggedTensor [[b'e']]>\n",
            "tf.Tensor([[6]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-13.855045    -3.3022397   -0.8141077   -1.2062961   -0.17424321\n",
            "   -0.5055839    0.21014467  -1.8377424   -0.84966874  -0.42243508\n",
            "   -0.31558996  -1.893529    -1.2362783    1.2988875    0.51901996\n",
            "    1.3009499    0.44122905   0.8868341   -2.7231154    1.5737127\n",
            "    1.7306104    0.48671773  -0.07700938  -1.0085512   -2.2087712\n",
            "   -1.4522187   -0.9421871   -2.1311953 ]], shape=(1, 28), dtype=float32)\n",
            "[19]\n",
            "<tf.RaggedTensor [[b'r']]>\n",
            "tf.Tensor([[19]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-14.599499     1.7202605    2.518543    -2.6529145   -2.1212134\n",
            "   -1.9656932    1.442919    -4.7754927   -1.8354521   -1.9313799\n",
            "    2.2760196   -4.9011083   -1.6018707   -0.55350477  -2.0070415\n",
            "    0.24730863   2.4657726   -1.8045192   -4.458078    -2.9736395\n",
            "   -0.5336878   -1.0396607    2.947944    -3.0768123   -5.585214\n",
            "   -2.6257782    0.60047376  -3.1338842 ]], shape=(1, 28), dtype=float32)\n",
            "[20]\n",
            "<tf.RaggedTensor [[b's']]>\n",
            "tf.Tensor([[20]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.922222     3.3873086    3.5913012   -2.272068     0.41525608\n",
            "   -1.1351376   -0.44606     -5.929862    -1.4398097   -0.8052802\n",
            "    0.30118203  -4.9426317   -0.6229373   -1.6500816   -1.2978677\n",
            "   -1.2194184   -0.02141809   0.6447394   -2.3076181   -3.5671203\n",
            "   -0.587002     1.2121404    1.5694895   -2.8460288   -6.041628\n",
            "   -3.0227447   -0.64159435  -1.606858  ]], shape=(1, 28), dtype=float32)\n",
            "[2]\n",
            "<tf.RaggedTensor [[b'a']]>\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.197617     0.9354166   -1.2216392   -0.38523966   0.5999329\n",
            "   -0.3640166   -0.5576194   -2.4707313   -1.089143    -1.864829\n",
            "   -1.0792776   -2.1646671   -1.33855      0.93801826  -0.23154995\n",
            "    1.9290271   -1.3502681    0.590977    -2.81739      0.9044548\n",
            "    0.37393165   1.3183277    2.24695     -0.1511141   -2.40418\n",
            "   -1.1641562   -2.8116326   -1.7073164 ]], shape=(1, 28), dtype=float32)\n",
            "[22]\n",
            "<tf.RaggedTensor [[b'u']]>\n",
            "tf.Tensor([[22]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-1.2161116e+01 -3.8603261e+00  1.1229653e+00 -1.4182469e+00\n",
            "   5.3376919e-01 -8.1922638e-01 -1.7297268e-03 -2.9635191e+00\n",
            "  -3.2978909e+00  6.0650003e-01 -4.9681973e-01 -4.0741353e+00\n",
            "  -1.0073954e+00  7.8386921e-01 -1.3137209e-01 -6.0228929e-02\n",
            "  -1.6640171e+00 -3.7713444e-01 -2.3042631e+00  3.1221511e+00\n",
            "   3.4416745e+00  7.4058604e-01  2.8342211e-01 -1.8877864e+00\n",
            "  -2.7571464e+00 -2.9800422e+00 -2.3175602e+00 -2.1210260e+00]], shape=(1, 28), dtype=float32)\n",
            "[20]\n",
            "<tf.RaggedTensor [[b's']]>\n",
            "tf.Tensor([[20]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.922222     3.3873086    3.5913012   -2.272068     0.41525608\n",
            "   -1.1351376   -0.44606     -5.929862    -1.4398097   -0.8052802\n",
            "    0.30118203  -4.9426317   -0.6229373   -1.6500816   -1.2978677\n",
            "   -1.2194184   -0.02141809   0.6447394   -2.3076181   -3.5671203\n",
            "   -0.587002     1.2121404    1.5694895   -2.8460288   -6.041628\n",
            "   -3.0227447   -0.64159435  -1.606858  ]], shape=(1, 28), dtype=float32)\n",
            "[2]\n",
            "<tf.RaggedTensor [[b'a']]>\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.197617     0.9354166   -1.2216392   -0.38523966   0.5999329\n",
            "   -0.3640166   -0.5576194   -2.4707313   -1.089143    -1.864829\n",
            "   -1.0792776   -2.1646671   -1.33855      0.93801826  -0.23154995\n",
            "    1.9290271   -1.3502681    0.590977    -2.81739      0.9044548\n",
            "    0.37393165   1.3183277    2.24695     -0.1511141   -2.40418\n",
            "   -1.1641562   -2.8116326   -1.7073164 ]], shape=(1, 28), dtype=float32)\n",
            "[22]\n",
            "<tf.RaggedTensor [[b'u']]>\n",
            "tf.Tensor([[22]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-1.2161116e+01 -3.8603261e+00  1.1229653e+00 -1.4182469e+00\n",
            "   5.3376919e-01 -8.1922638e-01 -1.7297268e-03 -2.9635191e+00\n",
            "  -3.2978909e+00  6.0650003e-01 -4.9681973e-01 -4.0741353e+00\n",
            "  -1.0073954e+00  7.8386921e-01 -1.3137209e-01 -6.0228929e-02\n",
            "  -1.6640171e+00 -3.7713444e-01 -2.3042631e+00  3.1221511e+00\n",
            "   3.4416745e+00  7.4058604e-01  2.8342211e-01 -1.8877864e+00\n",
            "  -2.7571464e+00 -2.9800422e+00 -2.3175602e+00 -2.1210260e+00]], shape=(1, 28), dtype=float32)\n",
            "[19]\n",
            "<tf.RaggedTensor [[b'r']]>\n",
            "tf.Tensor([[19]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-14.599499     1.7202605    2.518543    -2.6529145   -2.1212134\n",
            "   -1.9656932    1.442919    -4.7754927   -1.8354521   -1.9313799\n",
            "    2.2760196   -4.9011083   -1.6018707   -0.55350477  -2.0070415\n",
            "    0.24730863   2.4657726   -1.8045192   -4.458078    -2.9736395\n",
            "   -0.5336878   -1.0396607    2.947944    -3.0768123   -5.585214\n",
            "   -2.6257782    0.60047376  -3.1338842 ]], shape=(1, 28), dtype=float32)\n",
            "[16]\n",
            "<tf.RaggedTensor [[b'o']]>\n",
            "tf.Tensor([[16]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-15.078376    -2.394916    -1.128819    -0.8153734    1.2076231\n",
            "    1.3727484   -1.2957234   -2.6848135   -0.24540532  -1.2956328\n",
            "   -1.4852178   -1.8150665   -1.071228     1.2359458    0.79754174\n",
            "    2.350381    -1.7910467    1.8468916   -3.1194077    2.0386448\n",
            "    2.7928462    0.92800456  -1.6066785    0.16054189  -2.636657\n",
            "   -1.4418733   -2.2556834   -2.2305248 ]], shape=(1, 28), dtype=float32)\n",
            "[4]\n",
            "<tf.RaggedTensor [[b'c']]>\n",
            "tf.Tensor([[4]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-7.7605658  -2.6017766   0.659971   -2.190441   -0.94290507 -1.6498559\n",
            "   2.054521   -2.8305407  -1.8678122   2.7537715   0.88316596 -2.7076724\n",
            "  -0.73773515 -0.50973606 -1.382903   -1.9749492   2.136499    0.48858625\n",
            "  -1.7151809   1.192277    0.69939595  1.0347632  -0.23732    -3.3520098\n",
            "  -3.0255177  -4.09546     0.27722466 -2.294025  ]], shape=(1, 28), dtype=float32)\n",
            "[16]\n",
            "<tf.RaggedTensor [[b'o']]>\n",
            "tf.Tensor([[16]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-15.078376    -2.394916    -1.128819    -0.8153734    1.2076231\n",
            "    1.3727484   -1.2957234   -2.6848135   -0.24540532  -1.2956328\n",
            "   -1.4852178   -1.8150665   -1.071228     1.2359458    0.79754174\n",
            "    2.350381    -1.7910467    1.8468916   -3.1194077    2.0386448\n",
            "    2.7928462    0.92800456  -1.6066785    0.16054189  -2.636657\n",
            "   -1.4418733   -2.2556834   -2.2305248 ]], shape=(1, 28), dtype=float32)\n",
            "[5]\n",
            "<tf.RaggedTensor [[b'd']]>\n",
            "tf.Tensor([[5]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-17.00508     -7.966319     2.3702972   -3.1157815   -2.321515\n",
            "   -2.6451128    2.7756164   -4.453635    -3.7762356    1.1707456\n",
            "    1.9349319   -5.346246    -1.6693429    0.5706697   -1.1777714\n",
            "   -2.1287045    2.8672853   -0.7405098   -3.9406557    2.6096716\n",
            "    1.5643916    0.11799508   0.33014977  -3.9812393   -3.8268082\n",
            "   -4.4086914    1.4231981   -3.7356777 ]], shape=(1, 28), dtype=float32)\n",
            "[16]\n",
            "<tf.RaggedTensor [[b'o']]>\n",
            "tf.Tensor([[16]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-15.078376    -2.394916    -1.128819    -0.8153734    1.2076231\n",
            "    1.3727484   -1.2957234   -2.6848135   -0.24540532  -1.2956328\n",
            "   -1.4852178   -1.8150665   -1.071228     1.2359458    0.79754174\n",
            "    2.350381    -1.7910467    1.8468916   -3.1194077    2.0386448\n",
            "    2.7928462    0.92800456  -1.6066785    0.16054189  -2.636657\n",
            "   -1.4418733   -2.2556834   -2.2305248 ]], shape=(1, 28), dtype=float32)\n",
            "[20]\n",
            "<tf.RaggedTensor [[b's']]>\n",
            "tf.Tensor([[20]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.922222     3.3873086    3.5913012   -2.272068     0.41525608\n",
            "   -1.1351376   -0.44606     -5.929862    -1.4398097   -0.8052802\n",
            "    0.30118203  -4.9426317   -0.6229373   -1.6500816   -1.2978677\n",
            "   -1.2194184   -0.02141809   0.6447394   -2.3076181   -3.5671203\n",
            "   -0.587002     1.2121404    1.5694895   -2.8460288   -6.041628\n",
            "   -3.0227447   -0.64159435  -1.606858  ]], shape=(1, 28), dtype=float32)\n",
            "[20]\n",
            "<tf.RaggedTensor [[b's']]>\n",
            "tf.Tensor([[20]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.922222     3.3873086    3.5913012   -2.272068     0.41525608\n",
            "   -1.1351376   -0.44606     -5.929862    -1.4398097   -0.8052802\n",
            "    0.30118203  -4.9426317   -0.6229373   -1.6500816   -1.2978677\n",
            "   -1.2194184   -0.02141809   0.6447394   -2.3076181   -3.5671203\n",
            "   -0.587002     1.2121404    1.5694895   -2.8460288   -6.041628\n",
            "   -3.0227447   -0.64159435  -1.606858  ]], shape=(1, 28), dtype=float32)\n",
            "[1]\n",
            "<tf.RaggedTensor [[b'\\n']]>\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-7.0054893  -3.4786339   0.58242464  0.27924868  1.0223589   0.45471132\n",
            "   0.3861202  -0.29105878  0.72620726 -0.02788016 -0.8643983   0.17599824\n",
            "   0.27783486  0.01075115  0.8604859   0.12017788 -0.18005173  1.4274062\n",
            "  -0.60252404  0.23331997  1.5764735   1.1057353  -1.3854105  -0.73246586\n",
            "  -0.63410294  0.614537   -0.7077815  -0.36003038]], shape=(1, 28), dtype=float32)\n",
            "[24]\n",
            "<tf.RaggedTensor [[b'w']]>\n",
            "tf.Tensor([[24]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-14.274456    -0.12424687   3.2945836   -2.2289484   -2.2315183\n",
            "   -2.2470448    2.4246726   -4.7511      -4.914075    -1.5126463\n",
            "    2.3018484   -6.2128377   -1.5038925   -0.3526853   -1.8834039\n",
            "   -1.314454     1.1523908   -1.3890198   -4.307873    -1.4666338\n",
            "    0.1505112   -0.97855586   3.047021    -3.7702022   -5.3502135\n",
            "   -2.7412014    0.3310851   -3.7454307 ]], shape=(1, 28), dtype=float32)\n",
            "[6]\n",
            "<tf.RaggedTensor [[b'e']]>\n",
            "tf.Tensor([[6]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-13.855045    -3.3022397   -0.8141077   -1.2062961   -0.17424321\n",
            "   -0.5055839    0.21014467  -1.8377424   -0.84966874  -0.42243508\n",
            "   -0.31558996  -1.893529    -1.2362783    1.2988875    0.51901996\n",
            "    1.3009499    0.44122905   0.8868341   -2.7231154    1.5737127\n",
            "    1.7306104    0.48671773  -0.07700938  -1.0085512   -2.2087712\n",
            "   -1.4522187   -0.9421871   -2.1311953 ]], shape=(1, 28), dtype=float32)\n",
            "[19]\n",
            "<tf.RaggedTensor [[b'r']]>\n",
            "tf.Tensor([[19]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-14.599499     1.7202605    2.518543    -2.6529145   -2.1212134\n",
            "   -1.9656932    1.442919    -4.7754927   -1.8354521   -1.9313799\n",
            "    2.2760196   -4.9011083   -1.6018707   -0.55350477  -2.0070415\n",
            "    0.24730863   2.4657726   -1.8045192   -4.458078    -2.9736395\n",
            "   -0.5336878   -1.0396607    2.947944    -3.0768123   -5.585214\n",
            "   -2.6257782    0.60047376  -3.1338842 ]], shape=(1, 28), dtype=float32)\n",
            "[22]\n",
            "<tf.RaggedTensor [[b'u']]>\n",
            "tf.Tensor([[22]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-1.2161116e+01 -3.8603261e+00  1.1229653e+00 -1.4182469e+00\n",
            "   5.3376919e-01 -8.1922638e-01 -1.7297268e-03 -2.9635191e+00\n",
            "  -3.2978909e+00  6.0650003e-01 -4.9681973e-01 -4.0741353e+00\n",
            "  -1.0073954e+00  7.8386921e-01 -1.3137209e-01 -6.0228929e-02\n",
            "  -1.6640171e+00 -3.7713444e-01 -2.3042631e+00  3.1221511e+00\n",
            "   3.4416745e+00  7.4058604e-01  2.8342211e-01 -1.8877864e+00\n",
            "  -2.7571464e+00 -2.9800422e+00 -2.3175602e+00 -2.1210260e+00]], shape=(1, 28), dtype=float32)\n",
            "[20]\n",
            "<tf.RaggedTensor [[b's']]>\n",
            "tf.Tensor([[20]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.922222     3.3873086    3.5913012   -2.272068     0.41525608\n",
            "   -1.1351376   -0.44606     -5.929862    -1.4398097   -0.8052802\n",
            "    0.30118203  -4.9426317   -0.6229373   -1.6500816   -1.2978677\n",
            "   -1.2194184   -0.02141809   0.6447394   -2.3076181   -3.5671203\n",
            "   -0.587002     1.2121404    1.5694895   -2.8460288   -6.041628\n",
            "   -3.0227447   -0.64159435  -1.606858  ]], shape=(1, 28), dtype=float32)\n",
            "[2]\n",
            "<tf.RaggedTensor [[b'a']]>\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.197617     0.9354166   -1.2216392   -0.38523966   0.5999329\n",
            "   -0.3640166   -0.5576194   -2.4707313   -1.089143    -1.864829\n",
            "   -1.0792776   -2.1646671   -1.33855      0.93801826  -0.23154995\n",
            "    1.9290271   -1.3502681    0.590977    -2.81739      0.9044548\n",
            "    0.37393165   1.3183277    2.24695     -0.1511141   -2.40418\n",
            "   -1.1641562   -2.8116326   -1.7073164 ]], shape=(1, 28), dtype=float32)\n",
            "[22]\n",
            "<tf.RaggedTensor [[b'u']]>\n",
            "tf.Tensor([[22]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-1.2161116e+01 -3.8603261e+00  1.1229653e+00 -1.4182469e+00\n",
            "   5.3376919e-01 -8.1922638e-01 -1.7297268e-03 -2.9635191e+00\n",
            "  -3.2978909e+00  6.0650003e-01 -4.9681973e-01 -4.0741353e+00\n",
            "  -1.0073954e+00  7.8386921e-01 -1.3137209e-01 -6.0228929e-02\n",
            "  -1.6640171e+00 -3.7713444e-01 -2.3042631e+00  3.1221511e+00\n",
            "   3.4416745e+00  7.4058604e-01  2.8342211e-01 -1.8877864e+00\n",
            "  -2.7571464e+00 -2.9800422e+00 -2.3175602e+00 -2.1210260e+00]], shape=(1, 28), dtype=float32)\n",
            "[6]\n",
            "<tf.RaggedTensor [[b'e']]>\n",
            "tf.Tensor([[6]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-13.855045    -3.3022397   -0.8141077   -1.2062961   -0.17424321\n",
            "   -0.5055839    0.21014467  -1.8377424   -0.84966874  -0.42243508\n",
            "   -0.31558996  -1.893529    -1.2362783    1.2988875    0.51901996\n",
            "    1.3009499    0.44122905   0.8868341   -2.7231154    1.5737127\n",
            "    1.7306104    0.48671773  -0.07700938  -1.0085512   -2.2087712\n",
            "   -1.4522187   -0.9421871   -2.1311953 ]], shape=(1, 28), dtype=float32)\n",
            "[14]\n",
            "<tf.RaggedTensor [[b'm']]>\n",
            "tf.Tensor([[14]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-1.5246494e+01 -7.6365530e-01  2.7685966e+00 -2.8835216e+00\n",
            "  -3.0082850e+00 -2.4837086e+00  2.7222447e+00 -5.5249467e+00\n",
            "  -4.0884905e+00 -1.3189305e+00  2.8154950e+00 -6.2290683e+00\n",
            "  -1.8879656e+00 -4.8719695e-01 -2.6676188e+00 -9.9436522e-01\n",
            "   2.5557559e+00 -1.7893754e+00 -5.0378141e+00 -1.1069068e+00\n",
            "   3.2647252e-03 -1.1210387e+00  2.8416202e+00 -3.9462562e+00\n",
            "  -5.5709085e+00 -3.6869779e+00  9.6158445e-01 -4.2362137e+00]], shape=(1, 28), dtype=float32)\n",
            "[2]\n",
            "<tf.RaggedTensor [[b'a']]>\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.197617     0.9354166   -1.2216392   -0.38523966   0.5999329\n",
            "   -0.3640166   -0.5576194   -2.4707313   -1.089143    -1.864829\n",
            "   -1.0792776   -2.1646671   -1.33855      0.93801826  -0.23154995\n",
            "    1.9290271   -1.3502681    0.590977    -2.81739      0.9044548\n",
            "    0.37393165   1.3183277    2.24695     -0.1511141   -2.40418\n",
            "   -1.1641562   -2.8116326   -1.7073164 ]], shape=(1, 28), dtype=float32)\n",
            "[22]\n",
            "<tf.RaggedTensor [[b'u']]>\n",
            "tf.Tensor([[22]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-1.2161116e+01 -3.8603261e+00  1.1229653e+00 -1.4182469e+00\n",
            "   5.3376919e-01 -8.1922638e-01 -1.7297268e-03 -2.9635191e+00\n",
            "  -3.2978909e+00  6.0650003e-01 -4.9681973e-01 -4.0741353e+00\n",
            "  -1.0073954e+00  7.8386921e-01 -1.3137209e-01 -6.0228929e-02\n",
            "  -1.6640171e+00 -3.7713444e-01 -2.3042631e+00  3.1221511e+00\n",
            "   3.4416745e+00  7.4058604e-01  2.8342211e-01 -1.8877864e+00\n",
            "  -2.7571464e+00 -2.9800422e+00 -2.3175602e+00 -2.1210260e+00]], shape=(1, 28), dtype=float32)\n",
            "[19]\n",
            "<tf.RaggedTensor [[b'r']]>\n",
            "tf.Tensor([[19]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-14.599499     1.7202605    2.518543    -2.6529145   -2.1212134\n",
            "   -1.9656932    1.442919    -4.7754927   -1.8354521   -1.9313799\n",
            "    2.2760196   -4.9011083   -1.6018707   -0.55350477  -2.0070415\n",
            "    0.24730863   2.4657726   -1.8045192   -4.458078    -2.9736395\n",
            "   -0.5336878   -1.0396607    2.947944    -3.0768123   -5.585214\n",
            "   -2.6257782    0.60047376  -3.1338842 ]], shape=(1, 28), dtype=float32)\n",
            "[10]\n",
            "<tf.RaggedTensor [[b'i']]>\n",
            "tf.Tensor([[10]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-1.5540800e+01 -1.2052180e+00  1.2360468e+00 -1.4519612e+00\n",
            "   6.9993544e-01 -3.8009828e-01 -1.1123321e+00 -3.6200485e+00\n",
            "  -2.2963610e+00 -1.6894521e+00 -6.0304201e-01 -3.9907916e+00\n",
            "  -1.4114708e+00  8.1773430e-01 -8.4741116e-03  1.5689811e+00\n",
            "  -1.2471662e+00  4.1503176e-02 -3.3849101e+00  1.0620505e+00\n",
            "   1.5206974e+00  2.2932589e-01  1.0252259e+00 -9.8968208e-01\n",
            "  -3.8951883e+00 -2.3228254e+00 -2.1793833e+00 -2.2558656e+00]], shape=(1, 28), dtype=float32)\n",
            "[15]\n",
            "<tf.RaggedTensor [[b'n']]>\n",
            "tf.Tensor([[15]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-8.155976    1.1496396   1.3120141  -2.0969398  -0.57633686 -0.75516164\n",
            "   0.9588535  -2.6226768   1.8043219  -0.28766727  1.6556569  -1.4435362\n",
            "  -0.37079296 -0.95188653 -1.0807632   0.36082187  2.1566477  -1.1884615\n",
            "  -2.0435061  -2.7895901  -0.11317265  0.03925556  0.29689583 -2.2678285\n",
            "  -3.6995893  -1.325242    0.6385932  -1.390274  ]], shape=(1, 28), dtype=float32)\n",
            "[16]\n",
            "<tf.RaggedTensor [[b'o']]>\n",
            "tf.Tensor([[16]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-15.078376    -2.394916    -1.128819    -0.8153734    1.2076231\n",
            "    1.3727484   -1.2957234   -2.6848135   -0.24540532  -1.2956328\n",
            "   -1.4852178   -1.8150665   -1.071228     1.2359458    0.79754174\n",
            "    2.350381    -1.7910467    1.8468916   -3.1194077    2.0386448\n",
            "    2.7928462    0.92800456  -1.6066785    0.16054189  -2.636657\n",
            "   -1.4418733   -2.2556834   -2.2305248 ]], shape=(1, 28), dtype=float32)\n",
            "[20]\n",
            "<tf.RaggedTensor [[b's']]>\n",
            "tf.Tensor([[20]], shape=(1, 1), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[-10.922222     3.3873086    3.5913012   -2.272068     0.41525608\n",
            "   -1.1351376   -0.44606     -5.929862    -1.4398097   -0.8052802\n",
            "    0.30118203  -4.9426317   -0.6229373   -1.6500816   -1.2978677\n",
            "   -1.2194184   -0.02141809   0.6447394   -2.3076181   -3.5671203\n",
            "   -0.587002     1.2121404    1.5694895   -2.8460288   -6.041628\n",
            "   -3.0227447   -0.64159435  -1.606858  ]], shape=(1, 28), dtype=float32)\n",
            "[2]\n",
            "dievosanoprarsprus\n",
            "ersausaurocodoss\n",
            "werusauemaurinosa \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}